
% !TeX program = pdflatex
\documentclass[12pt,a4paper,oneside]{scrreprt}

% ---------- Packages ----------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tabularx}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
\usepackage{array}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{csquotes}
\usepackage[printonlyused]{acronym}
\usepackage{tocbasic} % better control of ToC lists
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{titling}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float}
\usepackage{pgf-pie}
\usepackage{siunitx}
\usepackage{pgfplots}
\usepackage[a4paper, left=3cm, right=2cm, top=2cm, bottom=3cm]{geometry}
\pgfplotsset{compat=1.18}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt}
\lstset{basicstyle=\ttfamily\small, numbers=left, numbersep=5pt, frame=single, breaklines=true}

% ---------- Page Setup ----------
\onehalfspacing
\setkomafont{sectioning}{\normalfont\bfseries}
\setkomafont{disposition}{\normalfont\bfseries}
\renewcommand{\arraystretch}{1.2}

% ---------- Header / Footer ----------
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{\nouppercase{\leftmark}} % aktueller Kapitelname
\fancyhead[R]{\includegraphics[height=1cm]{Bilder/TH_Nuernberg_Logo.png}} % passe den Pfad an!
\fancyfoot[C]{\thepage\ / \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% --- Optional: Stil für Kapitelanfangsseiten (ohne Logo) ---
\fancypagestyle{plain}{
  \fancyhead{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
}

% ---------- Metadata ----------
\newcommand{\arbeitstitel}{Entwicklung und Evaluierung von Algorithmen zur Umfelderkennung auf Basis hochauflösender 360°-LiDAR-Sensordaten}
\newcommand{\autor}{Ingrid Nodem Lambou}
\newcommand{\erstpruefer}{Prof. Dr. Christina Singer}
\newcommand{\zweitpruefer}{Prof. Dr. Christian Pfitzner}
\newcommand{\ort}{Nürnberg}
\newcommand{\abgabedatum}{30.11.2025}
\newcommand{\studiengang}{Mechatronik/Feinwerktechnik (B.\,Eng.)}
\newcommand{\matrikelnr}{3584699}
\newcommand{\studienschwerpunkt}{\textit{(eintragen)}} % Placeholder
\newcommand{\firma}{\textit{(eintragen)}} % Placeholder
\newcommand{\firmenbetreuer}{\textit{(Name, Abt., Tel.-Nr.)}} % Placeholder
\newcommand{\ausgabedatum}{\textit{(eintragen)}} % Placeholder

%-----------Literatur--------------
%-----------Literatur--------------
\usepackage[backend=biber,style=authoryear,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{literatur.bib} % nom exact du .bib

%--Document can have many numberedd subsection---
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3} % (optionnel : pour qu'elles apparaissent aussi dans la table des matières)

% --- TikZ für Flussdiagramme ---
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% --- Stile für Flowchart-Knoten und Pfeile ---
\tikzset{
  startstop/.style = {rectangle, rounded corners, minimum width=3.2cm,
    minimum height=1.0cm, text centered, draw=black, fill=red!30},
  process/.style   = {rectangle, minimum width=3.6cm, minimum height=1.0cm,
    text centered, draw=black, fill=orange!30},
  arrow/.style     = {thick,->,>=stealth}
}

% ---------- Document ----------
\begin{document}
\pagenumbering{Roman}

% ====== Offizielles Deckblatt (Hochschule) ======
\begin{titlepage}
    \centering
    \vspace*{1cm}
    {\Large\textbf{Bachelorarbeit}}\\[1.5cm]
    {\LARGE\textbf{\arbeitstitel}}\\[1.5cm]
    \begin{tabular}{@{}ll@{}}
        \textbf{Autor:} & \autor \\
        \textbf{Erstgutachter:} & \erstpruefer \\
        \textbf{Zweitgutachter:} & \zweitpruefer \\
        \textbf{Ort, Abgabetermin:} & \ort, \abgabedatum \\
    \end{tabular}
    \vfill
\end{titlepage}
\cleardoublepage

% ====== Offizielles Deckblatt (Formular) ======
\begin{titlepage}
    \centering
    {\Large\textbf{Offizielles Deckblatt Bachelorarbeit}}\\[1.2cm]
    \begin{tabular}{@{}p{5cm}p{9cm}@{}}
        \textbf{Bearbeiter:} & \autor \dotfill \\[0.3cm]
        \textbf{Matrikel-Nr.:} & \matrikelnr \dotfill \\[0.3cm]
        \textbf{Studiengang:} & \studiengang \dotfill \\[0.3cm]
        \textbf{Studienschwerpunkt:} & \studienschwerpunkt \dotfill \\[0.3cm]
        \textbf{Erstprüfer:} & \erstpruefer \dotfill \\[0.3cm]
        \textbf{Zweitprüfer:} & \zweitpruefer \dotfill \\[0.3cm]
        \textbf{Durchgeführt bei der Firma:} & \firma \dotfill \\[0.3cm]
        \textbf{Betreuer innerhalb der Firma:} & \firmenbetreuer \dotfill \\[0.3cm]
        \textbf{Ausgabedatum:} & \ausgabedatum \dotfill \\[0.3cm]
        \textbf{Abgabedatum:} & \abgabedatum \dotfill \\[0.8cm]
        \textbf{Thema der Arbeit:} & \arbeitstitel \\[0.8cm]
        \textbf{Die Arbeit ist frei einsehbar:} & $\Box$ Ja \quad $\Box$ Nein \\[0.8cm]
        \textbf{Die Arbeit darf nur mit Zustimmung von:} & \textit{(bei Firmenarbeiten Name, Abt., Tel.-Nr.)} \dotfill \\
    \end{tabular}
    \vfill
\end{titlepage}
\cleardoublepage

% ====== Prüfungsrechtliche Erklärung ======
\chapter*{Prüfungsrechtliche Erklärung}
Hiermit versichere ich, dass ich die vorliegende Bachelorarbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten oder nicht veröffentlichten Quellen entnommen wurden, sind als solche kenntlich gemacht. Diese Arbeit wurde in gleicher oder ähnlicher Form noch keiner anderen Prüfungsbehörde vorgelegt.\\[1.2cm]
\textbf{Nürnberg, \abgabedatum}\\[0.8cm]
\autor \dotfill
\cleardoublepage

% ====== Kurzfassung / Zusammenfassung ======
\chapter*{Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}
\noindent
\textit{Hier Kurzzusammenfassung in deutscher Sprache einfügen.} % Placeholder

\cleardoublepage

% ====== Inhalts- und Verzeichnisse ======
\tableofcontents
\cleardoublepage

\listoffigures
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\cleardoublepage

\listoftables
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\cleardoublepage

\chapter*{Abkürzungsverzeichnis}
\addcontentsline{toc}{chapter}{Abkürzungsverzeichnis}
\begin{acronym}[LiDAR]
    \acro{LiDAR}{Light Detection and Ranging}
    \acro{ROS}{Robot Operating System}
    \acro{ToF}{Time of Flight}
    \acro{BEV}{Bird's Eye View}
    \acro{IoU}{Intersection over Union}
    \acro{KF}{Kalman-Filter}
    \acro{CV}{Constant Velocity}
    \acro{TTL}{Time To Live}
    \acro{EMA}{Exponentielle gleitende Mittelung}
    \acro{QoS}{Quality of Service}
    \acro{MOTA}{Multiple Object Tracking Accuracy}
    \acro{MOTP}{Multiple Object Tracking Precision}
    \acro{IDF1}{ID-based F1-Score}
    \acro{HOTA}{Higher Order Tracking Accuracy}
    \acro{RViz}{ROS Visualization Tool}
\end{acronym}


\cleardoublepage

\pagenumbering{arabic}

% ====== 1 Einleitung ======
\chapter{Einleitung}

\section{Hinführung}

Autonome und hochautomatisierte Fahrfunktionen sind auf eine präzise, robuste und nachvollziehbare Umfelderfassung angewiesen, um sicherheitskritische Entscheidungen treffen zu können. Während Kameras vor allem für semantische Informationen genutzt werden und Radar Distanz sowie Relativgeschwindigkeit zuverlässig erfasst, liefert LiDAR dichte, dreidimensionale Punktwolken mit hoher Winkelauflösung und weitgehend konstanter Leistungsfähigkeit bei unterschiedlichen Lichtverhältnissen \parencite{Arnold2019Survey}. Dadurch bildet LiDAR in modernen Sensorarchitekturen einen zentralen Baustein der Umfeldwahrnehmung.

Im Projekt \textit{CarCeptionX} wird ein speziell ausgestattetes Versuchsfahrzeug eingesetzt, um die Leistungsfähigkeit automobiler Umfeldsensorik systematisch zu analysieren und neue Einsatzpotenziale zu erschließen (\cite{IFZN_Projekte}). Das Ziel des Projekts ist es, auf Basis realer Messdaten sowohl die sensorische Leistungsgrenzen als auch mögliche Optimierungspotenziale zu identifizieren, um langfristig einen Beitrag zu einer sicheren, effizienten und nachhaltigen Mobilität zu leisten. Der verwendete Versuchsträger (Opel Astra) ist hierfür mit einem umfassenden Sensorsetup ausgestattet, bestehend aus:

\begin{itemize}
    \item Ouster OS1 360°-LiDAR,
    \item Valeo Scala~2 LiDAR,
    \item Bosch General Purpose Radar,
    \item Valeo Fisheye-Kamera,
    \item Valeo Ultraschallsensor-Kit (12~Sensoren).
\end{itemize}

Am Institut für Fahrzeugtechnik Nürnberg (IFZN) wurde die LiDAR-basierte Umfelderfassung bereits in mehreren Arbeiten untersucht. Einen wesentlichen Grundstein legte die Masterarbeit von Wendel~(2025), in der eine Sensorplattform auf dem Opel Astra aufgebaut und eine erste ROS\,2-basierte Messkette zur Erfassung und Visualisierung von Punktwolken entwickelt wurde \parencite{Wendel2025}. Diese bestehende Infrastruktur – bestehend aus Datenerfassung, ROS\,2-Publish/Subscribe-Architektur, MATLAB-Anbindung über eine GUI sowie Visualisierung in \textit{RViz2} – bildet das Fundament der vorliegenden Arbeit.

Die anschließende Arbeit von Sagdic~(2025) erweiterte diese Grundlage um die vollständige Anbindung und initiale Inbetriebnahme des Ouster~OS1 auf dem Versuchsträger und dokumentierte die grundlegenden Funktions- und Schnittstellenprüfungen \parencite{Sagdic2025}. Auf dieser Basis setzt die vorliegende Arbeit auf, indem sie die bisher vorhandene Messkette um ein vollständiges, echtzeitfähiges Verarbeitungssystem ergänzt. 

Im Mittelpunkt steht dabei nicht der Sensor selbst, sondern die systematische Entwicklung, Implementierung und Bewertung der einzelnen Verarbeitungsschritte, die notwendig sind, um aus den Rohpunktwolken des Ouster~OS1 eine robuste Objekterkennung abzuleiten. Hierzu gehören die Vorverarbeitung der Punktwolken, eine zuverlässige Bodensegmentierung, die Clusterbildung zur Objektabgrenzung sowie die anschließende zeitliche Verfolgung der detektierten Objekte. Ziel ist es, die gesamte Verarbeitungskette so auszugestalten, dass sie unter realen urbanen Messbedingungen zuverlässig funktioniert und im Rahmen zukünftiger Forschungsaktivitäten wie \textit{CarCeptionX} nahtlos einsetzbar ist.

Zur Umsetzung dieser modularen und reproduzierbaren Verarbeitung wird das Robot Operating System~2 (ROS~2) eingesetzt, das mit standardisierten Nachrichtenformaten, deterministischen Kommunikationsmechanismen und integrierten Visualisierungstools ein etabliertes Softwareframework für mobile Robotersysteme darstellt \parencite{Macenski2022ROS2,ROS2Docs}. Die vorliegende Arbeit erweitert die bestehende Messkette des OS1 um zusätzlich implementierte Filter-, Segmentierungs- und Trackingmodule, welche die Punktwolke schrittweise in semantisch verwertbare Objektlisten überführen und damit einen funktionsfähigen Beitrag zur Weiterentwicklung des Versuchsträgers leisten.

\section{Themenspezifizierung und Abgrenzung}

Ziel dieser Arbeit ist die Entwicklung und Evaluierung einer modularen, echtzeitfähigen Verarbeitungskette zur LiDAR-basierten Umfelderkennung auf Basis des hochauflösenden Ouster~OS1. Unter realen Bedingungen soll diese Verarbeitungskette spezifische Verkehrsteilnehmer ab einer Größe von~$\geq 0{,}5\,\text{m}$ mit einer Detektionsrate von mindestens $90\,\%$ bei einer minimalen Ausführungsrate von $10\,\text{Hz}$ erkennen.

 Im Mittelpunkt stehen dabei mehrere klar abgegrenzte Arbeitsschritte: die qualitätssteigernde Vorverarbeitung der Rohpunktwolken zur Reduktion von Rauschen und Ausreißern sowie zur effizienten Datenreduktion, eine verlässliche und szenariounabhängige Bodensegmentierung, die eine stabile Trennung zwischen befahrbarer Fläche und Hindernissen ermöglicht, die extraktionssichere Bildung von Clustern und konsistenten Bounding-Boxen zur Objektabgrenzung sowie die anschließende zeitliche Verfolgung der erkannten Objekte. Die Implementierung erfolgt vollständig in ROS~2 mit dem Anspruch, eindeutig definierte Schnittstellen, reproduzierbare Experimente und messbare Leistungskennzahlen hinsichtlich Latenz, Frequenz und Genauigkeit zu gewährleisten.

Nicht Bestandteil der Arbeit sind Themenfelder, die zusätzliche Sensorquellen, umfangreiche Trainingsdaten oder andere Zielsetzungen erfordern. Dazu zählen insbesondere Sensorfusion mit Radar- oder Kamerasystemen, tiefenlernbasierte semantische Segmentierung, hardwareseitige Sensorauslegung sowie herstellerübergreifende Performancevergleiche. Ebenfalls ausgeschlossen wird das Gebiet des \textit{Simultaneous Localization and Mapping} (SLAM). SLAM umfasst die gleichzeitige Schätzung der Fahrzeugpose und den Aufbau einer konsistenten Umgebungskarte. Obwohl LiDAR hierfür ein zentrales Sensormedium darstellt, verfolgt diese Arbeit weder den Aufbau globaler Karten noch die Minimierung von Lokalisierungsfehlern. Der Fokus liegt ausschließlich auf der lokalen Umfeldwahrnehmung zur Objekterkennung, die ohne globale Konsistenzbedingungen oder kartengestützte Optimierung auskommt; die Kalibrierung beschränkt sich daher auf die notwendige Extrinsik und Zeitsynchronisation.

\section{Beitrag der Arbeit, Vorgehen und Aufbau}

Die Arbeit liefert einen wissenschaftlich und technisch fundierten Beitrag zur LiDAR-basierten Umfelderfassung, indem eine modular aufgebaute und echtzeitfähige ROS~2-Verarbeitungsstrecke entwickelt, implementiert und systematisch untersucht wird. Der Schwerpunkt liegt auf (i) einer qualitätssteigernden Vorverarbeitung, (ii) einer präzisen Trennung von Boden- und Objektpunkten, (iii) einer verlässlichen Cluster- und Bounding-Box-Ermittlung sowie (iv) einer konsistenten Objektverfolgung. Ein weiterer Beitrag besteht in der strukturierten Gegenüberstellung verschiedener Boden- und Clusterverfahren anhand nachvollziehbarer Metriken wie Punktzahlreduktion, Stabilität der Segmentierung, Clustertrennschärfe und zeitlichem Aufwand. Sämtliche Experimente werden reproduzierbar dokumentiert und mit klar definierten Parametern durchgeführt.

Die methodische Herangehensweise integriert diese Beiträge unmittelbar in den Aufbau der Arbeit. Sie beginnt mit einer systematischen Literaturauswertung und Ableitung funktionaler Anforderungen. Darauf folgt die gezielte Datenerhebung mit dem Ouster~OS1 sowie die Implementierung der einzelnen Funktionsbausteine der Verarbeitungsstrecke. Anschließend werden diese Bausteine in ROS~2 integriert und hinsichtlich Ausführungsmodell, Datendurchsatz und QoS-Profilen abgestimmt. In der experimentellen Phase werden die Verfahren unter definierten Szenarien untersucht, wobei insbesondere Segmentierungsgenauigkeit, räumliche Trennschärfe, Punktverluste, Stabilität der Objektverfolgung sowie der zeitliche Aufwand pro Verarbeitungsschritt analysiert werden. Dieser integrierte Ansatz ermöglicht eine durchgängige Bewertung der im Rahmen der Arbeit entwickelten Komponenten.

Abbildung~\ref{fig:methodik} zeigt die erweiterte methodische Vorgehensweise, in der die inhaltlichen Beiträge der Arbeit direkt verortet sind.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=7mm, >=latex]
\node (lit) [rectangle, draw, rounded corners, align=center, padding=3pt]
    {Literaturrecherche \\ \& Anforderungsanalyse};
\node (data) [rectangle, draw, rounded corners, below=of lit, align=center, padding=3pt]
    {Datenerhebung \\ mit Ouster OS1};
\node (impl) [rectangle, draw, rounded corners, below=of data, align=center, padding=3pt]
    {Implementierung der \\ Verarbeitungsbausteine};
\node (integ) [rectangle, draw, rounded corners, below=of impl, align=center, padding=3pt]
    {Integration in ROS\,2};
\node (eval) [rectangle, draw, rounded corners, below=of integ, align=center, padding=3pt]
    {Experimentelle Evaluierung \\ nach definierten Metriken};
\node (disc) [rectangle, draw, rounded corners, below=of eval, align=center, padding=3pt]
    {Vergleich der Verfahren \\ \& Analyse der Ergebnisse};

\draw[->] (lit) -- (data);
\draw[->] (data) -- (impl);
\draw[->] (impl) -- (integ);
\draw[->] (integ) -- (eval);
\draw[->] (eval) -- (disc);
\end{tikzpicture}
\caption{Methodische Vorgehensweise und Einbettung der Beiträge der Arbeit}
\label{fig:methodik}
\end{figure}

Zur Orientierung zeigt Abbildung~\ref{fig:aufbau} den strukturellen Aufbau der Arbeit. Kapitel~2 stellt den Stand der Technik vor. Kapitel~3 beschreibt die Systemarchitektur und die technische Einbettung der entwickelten Verarbeitungsstrecke. Kapitel~4 widmet sich der Vorverarbeitung der Punktwolken, gefolgt von der Bodensegmentierung in Kapitel~5. Kapitel~6 behandelt die Cluster-Extraktion und die Ermittlung von Bounding-Boxes. Kapitel~7 beschreibt die Objektverfolgung. Kapitel~8 umfasst die Tests und die Bewertung der Ergebnisse, bevor Kapitel~9 die Erkenntnisse zusammenfasst und einen Ausblick gibt.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=6mm, >=latex]
\node (k2) [rectangle, draw, rounded corners, padding=3pt] {Kapitel 2: Stand der Technik};
\node (k3) [rectangle, draw, rounded corners, below=of k2, padding=3pt] {Kapitel 3: Systemarchitektur};
\node (k4) [rectangle, draw, rounded corners, below=of k3, padding=3pt] {Kapitel 4: Vorverarbeitung};
\node (k5) [rectangle, draw, rounded corners, below=of k4, padding=3pt] {Kapitel 5: Bodensegmentierung};
\node (k6) [rectangle, draw, rounded corners, below=of k5, padding=3pt] {Kapitel 6: Cluster \& Bounding-Boxes};
\node (k7) [rectangle, draw, rounded corners, below=of k6, padding=3pt] {Kapitel 7: Objektverfolgung};
\node (k8) [rectangle, draw, rounded corners, below=of k7, padding=3pt] {Kapitel 8: Tests \& Bewertung};
\node (k9) [rectangle, draw, rounded corners, below=of k8, padding=3pt] {Kapitel 9: Fazit \& Ausblick};

\draw[->] (k2) -- (k3);
\draw[->] (k3) -- (k4);
\draw[->] (k4) -- (k5);
\draw[->] (k5) -- (k6);
\draw[->] (k6) -- (k7);
\draw[->] (k7) -- (k8);
\draw[->] (k8) -- (k9);
\end{tikzpicture}
\caption{Struktur der Arbeit}
\label{fig:aufbau}
\end{figure}



% ====== 2 Wissenschaftliche Grundlagen ======
\chapter{Stand der Technik und Wissenschaft}
\label{chap:stand-der-technik}

Die Umfeldwahrnehmung eines Fahrzeugs basiert in modernen 
Fahrerassistenzsystemen und im autonomen Fahren auf einer Vielzahl 
von Sensortechnologien. Neben Radar und Kamerasystemen 
hat sich insbesondere die LiDAR-Sensorik (\emph{Light Detection and Ranging}) 
als Schlüsseltechnologie etabliert (\cite{beuth2021handbuch}).

Das Akronym \emph{LiDAR} steht für „Light Detection and Ranging“ und 
bezeichnet alle optischen Messverfahren, die mittels elektromagnetischer 
Wellen einer Lichtquelle im ultravioletten, sichtbaren oder infraroten 
Spektralbereich Eigenschaften von Objekten durch Reflexion und 
Streuungsprozesse erfassen. LiDAR-Systeme senden kurze Lichtimpulse 
aus, messen die Laufzeit bis zur Reflexion und ermöglichen dadurch eine 
präzise Abstandsschätzung sowie die Rekonstruktion dreidimensionaler 
Strukturen der Umgebung (\cite{mcmanamon2015fieldguide}).

Die Entwicklung von LiDAR begann in den 1960er-Jahren, zunächst für 
Anwendungen in der Geodäsie, Fernerkundung und Atmosphärenforschung. 
Mit dem technologischen Fortschritt in der Lasertechnik sowie der 
Miniaturisierung optoelektronischer Komponenten fand die LiDAR-Sensorik 
in den 2000er-Jahren zunehmend Eingang in die Automobilindustrie. 
Heute gilt sie als eine der zentralen Technologien für Fahrerassistenzsysteme 
und hochautomatisiertes Fahren (\cite{beuth2021handbuch}).

Kameras liefern hochauflösende Bildinformationen, die für die 
Objekterkennung und Klassifizierung besonders geeignet sind, zeigen jedoch 
Schwächen bei schlechten Lichtverhältnissen oder starker Blendung. 
Radarsensoren dagegen arbeiten robust unter verschiedenen Wetterbedingungen 
und liefern präzise Informationen über die Relativgeschwindigkeit, verfügen 
jedoch nur über eine eingeschränkte Winkelauflösung. LiDAR kombiniert die 
Vorteile beider Systeme teilweise: Es liefert hochgenaue 
Distanzinformationen mit einer hohen Winkelauflösung und erzeugt 
dadurch eine dreidimensionale Punktwolke der Fahrzeugumgebung 
(\cite{beuth2021handbuch}).

Im Kontext des autonomen Fahrens ermöglicht LiDAR eine 
präzise Abbildung des Umfelds, die sowohl für die Hinderniserkennung 
als auch für die simultane Lokalisierung und Kartierung (SLAM) genutzt 
werden kann. Aufgrund seiner Eigenschaften wird LiDAR häufig als 
unverzichtbare Ergänzung zu Kamera und Radar betrachtet, da es zusätzliche Redundanz und Genauigkeit im Sensorsystemverbund schafft (\cite{beuth2021handbuch}).

Ziel dieses Kapitels ist die Darstellung der wissenschaftlich-technischen Grundlagen, relevanter Messprinzipien und Anwendungen sowie der in dieser Arbeit verwendeten Sensorik.

\section{Sensoraufbau}
\label{sec:sensoraufbau}

Ein LiDAR-Sensor umfasst Sendeoptik und Strahlablenkung, Empfangsoptik und Lichtsensor sowie Auslese- und Steuerungselektronik (vgl. Abb.~\ref{fig:sensoraufbau}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Bilder/Sensoraufbau.png}
    \caption{Schematischer Aufbau eines LiDAR-Sensors mit Sendeoptik, 
    Empfangsoptik, Strahlablenkung sowie Steuer- und Auswerteeinheit (\cite{beuth2021handbuch}).}
    \label{fig:sensoraufbau}
\end{figure}

Vom Ausgang der Laserquelle gelangt das Licht zunächst in die Sendeoptik. 
Diese formt das Intensitätsprofil des Strahls und bestimmt somit indirekt 
die erreichbare räumliche Auflösung des Sensors. Häufig werden refraktive 
Linsen oder diffraktive optische Elemente (DOE) eingesetzt, welche die 
gewünschte Strahlform erzeugen. Für rasternde Systeme übernehmen Spiegel 
oder Prismen die Strahlablenkung durch rotierende oder oszillierende 
Bewegungen. Im Gegensatz dazu nutzen sogenannte Flash-Systeme DOEs oder 
Diffusoren, um die Szene pro Aufnahme vollständig auszuleuchten (\cite{mcmanamon2015fieldguide}).

Die Strahlablenkung ist ein zentrales Element zur Erzeugung der Winkelinformation. 
Sie legt fest, welcher Raumwinkel zu einem bestimmten Zeitpunkt erfasst wird 
und ermöglicht so die sukzessive Abtastung der Umgebung. Die Genauigkeit der 
Ablenkmechanik und ihre Synchronisation bestimmen unmittelbar die geometrische 
Konsistenz des entstehenden Punktwolkenbildes (\cite{schuldt2020phd}).

Die Empfangsoptik übernimmt die Aufgabe, das vom Zielobjekt diffus oder spiegelnd 
zurückgestreute Licht möglichst verlustarm einzusammeln und dem Detektorsystem 
zuzuführen. Entscheidend dafür sind \emph{Aperturgröße} und \emph{Transmission}, 
die gemäß der LiDAR-Gleichung direkt auf die empfangene Strahlungsleistung wirken 
und damit die Reichweite sowie die Detektierbarkeit schwacher Signale bestimmen. 
Um Reflexionsverluste zu reduzieren, werden optische Komponenten mit 
Antireflexbeschichtungen (AR-Coatings) versehen. Ergänzend sorgen 
sperrbandselektive Filter -- deren Durchlässigkeit exakt auf die Laserwellenlänge 
abgestimmt ist -- für eine wirksame Unterdrückung von Umgebungslicht und erhöhen 
somit die Robustheit gegenüber Sonnenlicht (\cite{wolter2021optoelektronik}).

Als Detektoren kommen für automotive LiDAR-Systeme typischerweise 
Avalanche-Photodioden (APD) oder Single-Photon-Avalanche-Diodes (SPAD-Arrays) 
zum Einsatz. APDs bieten durch ihren internen Lawineneffekt eine hohe 
Verstärkung und eignen sich zur Detektion schwacher Rückstreusignale bei 
gleichzeitig guter zeitlicher Auflösung. SPAD-Arrays ermöglichen die Erfassung 
einzelner Photonen und erreichen extrem kurze Anstiegszeiten, wodurch sie eine 
sehr präzise Laufzeitmessung unterstützen. Diese Eigenschaften sind entscheidend 
für die Genauigkeit des Time-of-Flight-Prinzips (vgl. Kap.~\ref{sec:dTOF} und ~\ref{sec:iTOF} )und wirken sich direkt auf die 
präzise Rekonstruktion der Objektentfernung aus (\cite{lemkin2020spad}).

Die vom Detektor erzeugten elektrischen Signale werden anschließend durch die 
Ausleseelektronik verarbeitet. Diese umfasst Verstärker, Analog-Digital-Wandler 
(ADC), Zeit-zu-Digital-Wandler (TDC) und Multiplexer. Ihre Aufgabe besteht darin, 
die Signale zu digitalisieren und die Laufzeitinformationen für jeden 
Messpunkt zu bestimmen. Moderne Systeme berechnen bereits auf Sensorebene 
Distanzwerte und können weitergehende Vorverarbeitungen wie die Bildung von 
Objektboxen oder erste Klassifikationen durchführen (\cite{behroozpour2017adc}).

Eine zentrale Rolle spielt die Steuerungselektronik, welche die Synchronisation 
aller Komponenten sicherstellt. Sie sorgt dafür, dass der Detektor nur während 
des Aussendens eines Laserpulses aktiv ist, um unnötige Rauschsignale durch 
Umgebungslicht zu vermeiden. Ebenso steuert sie die Strahlablenkung, sodass 
jeder Messpunkt dem korrekten Raumwinkel zugeordnet werden kann (\cite{beuth2021handbuch}).

Der LiDAR-Sensoraufbau vereint optische, elektronische und algorithmische 
Bausteine zu einem Gesamtsystem, das die präzise Abbildung der Fahrzeugumgebung 
in Echtzeit ermöglicht (\cite{schuldt2020phd}).

\section{Wellenlängenbereiche}
\label{sec:wellenlaenge}
Zur Umsetzung dieses Prinzips werden überwiegend Laser im nahen Infrarotbereich 
(NIR) eingesetzt, typischerweise mit Wellenlängen zwischen $870\,\text{nm}$ und 
$950\,\text{nm}$. Dieser Bereich ist besonders geeignet, da die spektrale 
Strahlungsintensität des Sonnenlichts dort vergleichsweise gering ist und somit 
ein günstiges Signal-Rausch-Verhältnis erzielt werden kann. Gleichzeitig stehen in diesem Wellenlängenbereich sowohl kosteneffiziente Halbleiterlaser als auch empfindliche Siliziumdetektoren zur Verfügung. Für spezielle Anwendungen werden 
auch Wellenlängen um $1064\,\text{nm}$ oder $1550\,\text{nm}$ eingesetzt, da hier 
höhere Laserleistungen innerhalb der Augensicherheitsgrenzen möglich sind. Diese 
erfordern jedoch alternative Detektormaterialien, was den technischen Aufwand und 
die Kosten erhöht(\cite{beuth2021handbuch}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Bilder/Spektrum der elektromagnetischen Strahlung.png}
    \caption{Spektrum der elektromagnetischen Strahlung(\cite{beuth2021handbuch})}
    \label{fig:ouster}
\end{figure}

\section{Messprinzip}
\label{sec:messprinzip}

\subsection{Direct Time‑of‑Flight (dToF)}
\label{sec:dTOF}

Das Messprinzip moderner LiDAR-Systeme basiert überwiegend auf dem 
\emph{Time-of-Flight} (ToF)-Verfahren, das sich aufgrund seiner Robustheit und 
hohen Genauigkeit als Standard etabliert hat. Beim ToF-Verfahren wird ein kurzer 
Laserimpuls ausgesendet, der sich mit Lichtgeschwindigkeit 
$c \approx 3 \cdot 10^{8}\,\text{m/s}$ ausbreitet, von einem Objekt reflektiert wird und schließlich den Empfänger erreicht. Aus der gemessenen Zeitdifferenz $\Delta t$ zwischen Emission und Detektion ergibt sich die Entfernung $d$ nach folgender Gleichung:

\begin{equation}
    d = \frac{c \cdot \Delta t}{2}
\end{equation}

Der Faktor $\tfrac{1}{2}$ berücksichtigt, dass der Lichtimpuls den Weg zweimal zurücklegt – vom Sender zum Objekt und wieder zurück zum Empfänger.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/Schematisches Beispiel eines direct ToF.png}
    \caption{Schematisches Beispiel eines direct Time-Of-Flight (dToF) \parencite{beuth2021handbuch}
}
    \label{fig:ouster}
\end{figure}

\subsection{Indirect ToF (iToF)}
\label{sec:iTOF}

Beim iToF-Messprinzip wird die Entfernung nicht über die direkte Messung der Lichtlaufzeit bestimmt, sondern indirekt über die Veränderung eines kontinuierlich modulierten ausgesendeten Signals. Die zurückkehrende Reflexion weist abhängig von der Distanz charakteristische Änderungen auf, aus denen die Entfernung rekonstruiert werden kann. In der Praxis kommen hierfür hauptsächlich \emph{Amplitudenmodulationen} und \emph{Frequenzmodulationen} zum Einsatz \parencite{beuth2021handbuch}.

\subsubsection{AMCW (Amplitude-Modulated Continuous Wave)}

Bei einem AMCW‑LiDAR wird ein kontinuierlich ausgesendeter Lichtstrahl mit einer Amplitudenmodulation versehen, typischerweise sinus‑ oder rechteckförmig.  Das vom Objekt reflektierte Licht weist gegenüber dem ausgesendeten Signal eine Phasenverschiebung~$\Delta\phi$ auf, die proportional zur Laufzeit und damit zur Entfernung ist.  Die Distanz lässt sich aus der gemessenen Phasenverschiebung bestimmen.  Für die Entfernung $d$ ergibt sich in Abhängigkeit von der Modulationsfrequenz~$f_\mathrm{mod}$ des sinusförmig modulierten Strahls
\begin{equation}
    d = \frac{c}{4\pi f_\mathrm{mod}}\, \Delta\phi\,,
\end{equation}
wobei $c$ die Lichtgeschwindigkeit bezeichnet.  Die maximale eindeutige Messdistanz (unambiguous range) ist durch die Periodizität des Modulationssignals begrenzt und beträgt
\begin{equation}
    d_\text{max} = \frac{c}{2 f_\mathrm{mod}}\,,
\end{equation}
so dass eine eindeutige Zuordnung der gemessenen Phasenverschiebung nur innerhalb dieser Entfernung möglich ist.  Höhere Modulationsfrequenzen verbessern zwar die Tiefenauflösung, verkürzen aber die eindeutige Reichweite proportional dazu \parencite{yi2023digital}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Bilder/Schema_AMCW_LiDAR.png}
    \caption{Schema eines amplitudenmodulierten LiDARs (AMCW) \parencite{beuth2021handbuch}.}
    \label{fig:amcw_schema}
\end{figure}

AMCW‑Systeme weisen als Verfahren mehrere Vorteile auf.  Die benötigte Hardware ist vergleichsweise einfach und kostengünstig zu realisieren, was hochintegrierte Sensordesigns ermöglicht.  Durch den kontinuierlichen Betrieb können niedrigere Spitzenleistungen genutzt werden, und bei kurzen Messdistanzen wird eine hohe Tiefenauflösung erreicht.  Aus diesen Gründen sind AMCW‑Sensoren in industriellen 3D‑Kameras, Smartphones und mobilen Endgeräten weit verbreitet \parencite{yi2023digital}.

Dem stehen charakteristische Nachteile gegenüber.  Die periodische Modulation führt zu einer Entfernungsambiguität, da eindeutige Distanzmessungen nur bis zur oben genannten unambiguous range möglich sind.  Zudem reagieren AMCW‑Systeme empfindlich auf Multipath‑Effekte: Mehrfachreflexionen unterschiedlicher Laufzeit überlagern sich und verfälschen die gemessene Phase, was insbesondere bei glänzenden Materialien oder in Innenraumumgebungen problematisch ist.  Weitere Fehlerquellen entstehen durch Phasenverkettung (2\,$\pi$‑Ambiguität), Alias‑Effekte höherer Harmonischer und Bewegungsartefakte, die durch die mehrfache Abtastung pro Bild entstehen \parencite{whyte2015application}.

Insgesamt eignet sich das AMCW‑Verfahren vor allem für kostensensitive und kompakte Sensorsysteme mit begrenzten Messdistanzen.  Seine Stärken liegen in der einfachen Realisierbarkeit und der hohen Präzision auf kurzen Entfernungen, während Nachteile wie begrenzte eindeutige Reichweite und Empfindlichkeit gegenüber Mehrwegeffekten den Einsatz in komplexen oder stark reflektierenden Umgebungen einschränken \parencite{yi2023digital}.

\subsubsection{FMCW (Frequency Modulated Continuous Wave)}
\label{sec:fmcw_lidar}

Ein FMCW‑LiDAR nutzt kontinuierlich abgestrahlte Laserstrahlung, deren Frequenz zeitlich über einen sogenannten \emph{Chirp} linear variiert wird. Eine schematische Darstellung des grundlegenden Messprinzips ist in Abb.~\ref{fig:fmcw} gezeigt. Durch die Überlagerung des ausgesandten und des vom Objekt reflektierten Signals entsteht ein niederfrequentes \emph{Beat‑Signal}, dessen Frequenz proportional zur Laufzeitdifferenz ist. Aus diesem Beat‑Signal lässt sich die Entfernung mit hoher Präzision bestimmen\parencite{pinto2025fundamentals}.

Im Gegensatz zu klassischen Pulssystemen erlaubt FMCW zudem die gleichzeitige Ermittlung der Relativgeschwindigkeit: Eine durch die Objektbewegung verursachte Doppler‑Verschiebung verändert die Beat‑Frequenz, sodass Distanz‑ und Geschwindigkeitsinformation in einem einzigen Messvorgang separierbar sind. Dies führt zu einer besonders robusten Bewegungsanalyse, selbst bei schwachen Reflexionen oder geringem Signal‑Rausch‑Verhältnis\parencite{piggott2022physics}.

Ein zentraler Vorteil dieser Technik ist die hohe Störrobustheit gegenüber Fremdlicht und anderen LiDAR‑Quellen. Da FMCW auf kohärenter Detektion basiert, werden ausschließlich Signale mit identischer Modulationscharakteristik ausgewertet. Dadurch können selbst sehr schwache Rückstreusignale zuverlässig erfasst werden, was die Performance bei schlechten Sichtbedingungen verbessert. Zudem ermöglicht die phasenbasierte Auswertung eine hohe Entfernungsauflösung bis in den Millimeterbereich \parencite{piggott2022immunity}.

Demgegenüber erfordert FMCW‑LiDAR eine aufwändigere opto‑elektronische Architektur, einschließlich laserfrequenzstabiler Quellen, präziser Modulation, schneller Detektoren und leistungsfähiger digitaler Signalverarbeitung. Diese Anforderungen führen aktuell zu höheren Herstellungskosten sowie erhöhtem Rechenaufwand in der Echtzeit‑Auswertung \parencite{bianconi2025challenges}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Bilder/fmcw.png}
    \caption{Schematisches Messprinzip eines FMCW‑LiDARs (\cite{beuth2021handbuch}).}
    \label{fig:fmcw}
\end{figure}

\subsection{Fehlerquellen}
\label{sec:fehlerquellen}

Die Leistungsfähigkeit optischer LiDAR-Sensoren wird von mehreren systematischen und stochastischen Fehlerquellen beeinflusst. Eine zentrale Rolle spielen Schwankungen in der Zeitmessung (\emph{Zeitjitter}) innerhalb der TDC- bzw. ADC-Elektronik, die direkt die Laufzeitbestimmung und damit die Distanzauflösung beeinträchtigen \parencite{rapp2020lidar}. Weitere typische Fehlerursachen sind \emph{Mehrwegeffekte (Multipath)}, bei denen reflektiertes Licht über indirekte Wege zum Detektor gelangt und so zu verfälschten oder doppelten Entfernungswerten führt \parencite{glennie2013multichannel}. 

Auch der Einfallswinkel des Laserstrahls sowie die optische Reflektivität der Oberfläche wirken sich auf die Signalstärke aus: Flache Winkel oder dunkle, absorbierende Materialien reduzieren die Signalenergie und erhöhen damit das Rauschen bzw. die Wahrscheinlichkeit von Fehlmessungen \cite{beuth2021handbuch}. Zusätzlich begrenzt die Strahldivergenz des Lasers die räumliche Auflösung, da der Lichtfleck mit zunehmender Entfernung größer wird und Reflexionen von benachbarten Objekten überlagert werden können.

Neben optischen Einflüssen spielen auch umwelt- und systembedingte Faktoren eine Rolle. temperaturbedingte Messabweichungen im Lasersystem oder in der Empfangselektronik können den Messoffset verschieben oder die Verstärkung verändern. Mechanische Komponenten – insbesondere bei rotierenden LiDARs – unterliegen Toleranzen und Vibrationen, beispielsweise durch Schwankungen der Rotordrehzahl, was zu nichtlinearen Abtastwinkeln oder zeitlichen Versätzen führt (\cite{levinson2011towards}). Diese Effekte wirken sich unmittelbar auf Reichweite, Präzision und Datenkonsistenz aus und können nachfolgende Verarbeitungsschritte wie Segmentierung, Objekterkennung oder Klassifikation beeinträchtigen.

Um in sicherheitsrelevanten Anwendungen robuste Umfeldmodelle zu gewährleisten, sind daher geeignete Maßnahmen zur Signalkonditionierung, Filterung und Fehlerkompensation notwendig. Dazu gehören Rauschreduktion, adaptive Schwellenwertverfahren, geometrische Konsistenzprüfungen sowie modellbasierte Korrekturen, um messtechnische Artefakte zu minimieren und Fehlinterpretationen im Gesamtsystem zu vermeiden.

\section{Typische Anwendungsszenarien im Fahrzeugumfeld}
\label{sec:anwendung}

LiDAR-Sensoren spielen im Fahrzeugumfeld eine zentrale Rolle bei klassischen Fahrerassistenzsystemen (ADAS). Ein wichtiger Anwendungsbereich ist die \emph{Freiraum- und Belegtheitsdetektion}, bei der die Punktwolken genutzt werden, um befahrbare Bereiche zu bestimmen und Hindernisse im Nah- und Fernbereich präzise zu lokalisieren. Dies bildet die Grundlage für Funktionen wie automatische Notbremsung (AEB), Kollisionsvermeidung, Abstandsregelung sowie Querführung in niedrigen Automatisierungsstufen \parencite{Arnold2019Survey}.

Für höher automatisierte Fahrfunktionen (Level~3–4) dient LiDAR als wesentliche Komponente zur \emph{3D-Umfeldmodellierung}. Dabei werden aus den Punktwolken statische Elemente der Infrastruktur (z.\,B. Leitplanken, Begrenzungslinien, geparkte Fahrzeuge) sowie dynamische Verkehrsteilnehmer wie Fahrzeuge, Radfahrer oder Fußgänger extrahiert und klassifiziert. Die hohe geometrische Auflösung von LiDAR ermöglicht die Erstellung präziser Objektlisten, Konturen und Bewegungsmodelle, die von der Trajektorienplanung und Entscheidungslogik weiterverarbeitet werden \cite{beuth2021handbuch}.

Ein weiterer wichtiger Anwendungsbereich liegt in der \emph{Lokalisierung und Kartenerstellung (Mapping)}. LiDAR-basierte SLAM-Verfahren (Simultaneous Localization and Mapping) und Scan-Matching-Algorithmen nutzen die 3D-Punktwolken, um die Fahrzeugpose kontinuierlich zu schätzen und lokale Karten der Umgebung aufzubauen. Dies ist insbesondere in unbekannten oder unstrukturierten Gebieten relevant und dient als Ergänzung zu GPS, Inertialsensorik und Kamera \cite{cadena2016slam}.

Darüber hinaus wird LiDAR in komplexen und dynamischen Verkehrsszenarien eingesetzt, etwa im \emph{Baustellen- und Engstellenmanagement}. Dort ermöglicht die präzise 3D-Geometrie das Erkennen veränderter Fahrkorridore, temporärer Barrieren, verschwenkter Fahrspuren oder unstrukturierter Objektumgebungen. Diese Information bildet die Grundlage für robuste Pfadplanung, sichere Trajektoriengenerierung und automatisierte Spurwahl \cite{levinson2011towards}.

Schließlich unterstützt LiDAR Funktionen der \emph{Situations- und Kontextinterpretation}, etwa die Analyse von Verkehrsfluss, Abstandsdynamiken, Manövererkennung anderer Verkehrsteilnehmer sowie die Ableitung semantischer Strukturen der Umgebung (z.\,B. Fahrbahnränder, drivable space, Objekttypen). Die detaillierten 3D-Daten dienen dabei als hochpräzise geometrische Referenzschicht innerhalb der Sensorsuite \cite{mcauliffe2019scaling}.

\section{Algorithmischer Stand der Technik}
\label{sec:stand-der-technik}

Die algorithmische Verarbeitung hochauflösender 3D-LiDAR-Daten folgt in der Literatur einem weitgehend etablierten Verarbeitungskettenmodell, das aus Vorverarbeitung, Bodensegmentierung, Segmentierung (Clustering), Merkmalsextraktion sowie Objektverfolgung besteht. Jedes Modul adressiert dabei spezifische Herausforderungen roher Punktwolken wie Rauschen, variable Punktdichten, Mehrwegeffekte oder dynamische Verkehrsszenen. Im Folgenden werden die einzelnen Bausteine detailliert beschrieben und jeweils mit relevanter Literatur untermauert.

% =====================================================================
\subsection{Vorverarbeitung}
\label{subsec:vorverarbeitung}

Die Vorverarbeitung dient der Reduktion der Datenmenge, der Stabilisierung der Punktwolke sowie der Vorbereitung der Daten für die nachfolgenden Module. Häufig eingesetzte Schritte umfassen:
\paragraph{Downsampling}
Ein verbreitetes Verfahren zur kontrollierten Reduktion der Punktanzahl ist das \emph{VoxelGrid}-Downsampling. Die Punktwolke wird in dreidimensionale Voxel unterteilt, wobei alle Punkte eines Voxels durch ihren Schwerpunkt ersetzt werden. Dadurch wird die räumliche Auflösung gezielt skaliert, ohne wesentliche geometrische Strukturen zu verlieren \parencite{RusuCousins2011}.

\paragraph{Ausreißerentfernung}
Zur Rauschreduktion werden in der Praxis der \emph{Statistical Outlier Removal (SOR)} sowie der \emph{Radius Outlier Removal (ROR)} eingesetzt.
SOR entfernt Punkte, deren mittlere Distanz zu ihren Nachbarn stark vom Erwartungswert abweicht, während ROR Punkte eliminiert, die innerhalb eines definierten Radius zu wenige Nachbarn besitzen \parencite{RusuCousins2011}. In der vorliegenden Implementierung wurde auf beide Verfahren verzichtet, weil die Punktwolken des Ouster-Sensors im Testumfeld ausreichend stabil waren und die zusätzliche Rechenzeit nicht gerechtfertigt erschien.

\paragraph{Bewegungskompensation (Deskewing)}
Da LiDAR-Sensoren Punktwolken zeilenweise aufnehmen, entstehen bei Fahrzeugbewegungen Verzerrungen. Durch Deskewing unter Verwendung von IMU- oder Odometrie-Daten werden zeitbezogene Positionsfehler korrigiert \parencite{Behley2018}.

\paragraph{Intensitätsnormalisierung}
Die Intensität eines Punktes wird durch Materialeigenschaften und Auftreffwinkel beeinflusst. Eine Distanz- und Winkelkorrektur verbessert die Clusterqualität sowie nachfolgende Klassifikationsschritte \parencite{LevinsonThrun2011}.

\paragraph{Koordinatentransformationen}
Für bestimmte Verfahren werden die Punktwolken in Polarkoordinaten oder \emph{Bird’s-Eye-View}-Raster überführt, beispielsweise für scanline-basierte Segmentierer oder CNN-Detektoren \parencite{Lang2019PointPillars}.

\paragraph{Mehrfachechos}
Moderne LiDARs bieten mehrere Echos pro Laserpuls. Das erste Echo eignet sich zur Bodenerkennung, spätere Echos unterstützen die Erkennung hinter teiltransparenten Strukturen wie Vegetation \parencite{Glennie2010}.

% =====================================================================
\subsection{Bodensegmentierung}
\label{subsec:bodenseg}

Die Trennung von Boden und darüberliegenden Objekten ist ein entscheidender Schritt zur Reduktion des Suchraums. Zu den wichtigsten Verfahren gehören:

\paragraph{RANSAC-Ebene}
Eine globale Bodenebene kann mittels RANSAC robust geschätzt werden. Das Verfahren ist tolerant gegenüber Ausreißern, stößt jedoch bei geneigten Szenen oder unebenem Gelände an Grenzen \parencite{FischlerBolles1981}.

\paragraph{Progressive Morphological Filter (PMF)}
Der PMF nutzt morphologische Operationen auf einer Höhenkarte und wurde ursprünglich für Airborne-LiDAR entwickelt. Er ist besonders robust gegenüber hügeligem Gelände \parencite{Zhang2003PMF}.

\paragraph{Grid-basierte Höhenkarten}
Hierbei wird die Punktwolke in ein zweidimensionales Raster projiziert, aus dem pro Zelle statistische Höhenmerkmale extrahiert werden. Diese Methode ist effizient und gut für Echtzeitsysteme geeignet.

\paragraph{Cloth Simulation Filtering (CSF)}
Ein virtuelles Tuch wird auf die invertierte Punktwolke “fallen gelassen”, wodurch die Bodenform approximiert wird. CSF hat sich als robust gegenüber Vegetation und komplexen Geländeformen erwiesen \parencite{Zhang2016CSF}.

\paragraph{Probabilistische Modelle}
Gaussian-Process-Modelle modellieren lokale Höhenverteilungen probabilistisch und sind besonders geeignet für urbane Szenarien mit Bordsteinen \parencite{Vetter2017}.

% =====================================================================
\subsection{Clustering und Segmentierung}
\label{subsec:clustering}

Nach Entfernung des Bodens wird die verbleibende Punktwolke segmentiert. Wichtige Verfahren sind:

\paragraph{Euklidische Clusterung}
Das klassische PCL-Verfahren gruppiert Punkte, die innerhalb eines Distanzschwellwertes verbunden sind. Diese Methode ist einfach und schnell, reagiert jedoch empfindlich auf variable Punktdichten.

\paragraph{DBSCAN und HDBSCAN}
Dichtebasierte Verfahren erkennen Cluster beliebiger Form und klassifizieren isolierte Punkte als Rauschen. DBSCAN ist in vielen LiDAR-Verarbeitungsketten Standard \parencite{Ester1996DBSCAN}.  
HDBSCAN erweitert das Konzept um hierarchische Dichteanalysen und ist robuster gegenüber Dichteunterschieden.

\paragraph{Region Growing}
Unter Verwendung lokaler Normale und Krümmungen segmentiert Region Growing größere, glatte Flächen. Dies ist besonders nützlich für Infrastruktur und Fassaden.

% =====================================================================
\subsection{Detektion und Klassifikation}
\label{subsec:detektion}

Ziel dieses Moduls ist die Identifikation potenzieller Objekte.

\paragraph{Geometrische Merkmale}
Aus den Clustern werden typische Merkmale wie Achslängen, PCA-Formfaktoren, Punktdichte oder orientierte Bounding Boxes extrahiert. Diese Verfahren benötigen keine Trainingsdaten und sind für Echtzeit geeignet \parencite{Douillard2011}.

\paragraph{Lernbasierte 3D-Detektoren}
Moderne Methoden wie \emph{PointPillars}, \emph{SECOND} oder \emph{CenterPoint} repräsentieren Punktwolken als BEV-Raster und nutzen CNNs, um Fahrzeug-, Fußgänger- und Fahrradklassen zu erkennen \parencite{Lang2019PointPillars}.  
Diese Verfahren liefern hohe Genauigkeiten, erfordern jedoch umfangreiche Trainingsdaten und GPU-Ressourcen.

% =====================================================================
\subsection{Tracking}
\label{subsec:tracking}

Bewegte Objekte werden über mehrere Zeitschritte verfolgt. Dafür sind zwei Teilkomponenten notwendig:

\paragraph{Bewegungsmodelle}
Mehrere Filterverfahren kommen zum Einsatz:
\begin{itemize}
    \item Kalman-Filter (KF) für lineare Bewegungsmodelle,
    \item Extended Kalman Filter (EKF) für leicht nichtlineare Modelle,
    \item Unscented Kalman Filter (UKF) für stärker nichtlineare Bewegungen.
\end{itemize}
Sie modellieren Objektzustände wie Position und Geschwindigkeit \parencite{WelchBishop1995}.

\paragraph{Datenassoziation}
Um neue Detektionen bestehenden Objekten zuzuordnen, kommen Verfahren wie \emph{Nearest Neighbor}, \emph{Joint Probabilistic Data Association (JPDAF)} oder \emph{Multiple Hypothesis Tracking (MHT)} zum Einsatz.  
In vielen automotive-nahen Arbeiten wird der \emph{Ungarische Algorithmus} zur optimalen Zuordnung verwendet \parencite{Kuhn1955Hungarian}.

\paragraph{Track-Management}
Typische Regeln umfassen:
\begin{itemize}
    \item Erzeugung neuer Tracks ab stabiler Beobachtung,
    \item Löschen nicht bestätigter Tracks,
    \item Lebenszeitkriterien zur Validität.
\end{itemize}
Dies verhindert spontane Fehldetektionen und stabilisiert die Szeneinterpretation.

\section{Ouster OS1 am Opel Astra der TH Nürnberg}

\subsection{Spezifische Eigenschaften des Ouster OS1}
\label{sec:eigenschaften}

Der \emph{Ouster OS1} (vgl. Abbildung~\ref{fig:ouster_os1}) ist ein rotationssymmetrischer 360°-LiDAR, der eine hochauflösende dreidimensionale Erfassung der Umgebung ermöglicht. Die im Projekt verwendete Variante verfügt über eine vertikale Auflösung von 128 Kanälen. In der maximalen Konfiguration erreicht der Sensor eine horizontale Auflösung von 2{,}048 Abtastungen pro Umdrehung, was zu einer Punktwolke mit bis zu 262\,144 Messpunkten pro Scan führt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Bilder/Ouster.png}
    \caption{Ouster OS1 \parencite{sensalyticsRetailAnalytics}}
    \label{fig:ouster_os1}
\end{figure}

In der derzeitigen Konfiguration sind 1{,}024 horizontale Winkelkanäle aktiv, sodass pro Umdrehung rund 131\,072 Datenpunkte generiert werden. Über das Webinterface des Sensors lassen sich sämtliche relevanten Parameter flexibel anpassen, darunter die horizontale Auflösung (512, 1{,}024 oder 2{,}048 Kanäle) sowie die Framerate (10 bis 20\,Hz). Die Datenübertragung erfolgt über eine Gigabit-Ethernet-Schnittstelle, über die sowohl die UDP-Punktwolkendaten als auch Konfigurationsbefehle übertragen werden \parencite{Ouster2024}.

Weitere technische Details zur Integration und Konfiguration des Sensors sind ausführlich in \textcite{Sagdic2025} dokumentiert.


\begin{table}[htb]
    \centering
    \label{tab:ouster_os1_specs}
    \begin{tabular}{p{0.35\textwidth}p{0.55\textwidth}}
        \hline
        \textbf{Hersteller:}              & Ouster Inc. \\ 
        \textbf{Gerätebezeichnung:}       & OS1 Mid-Range Imaging LiDAR \\ 
        \textbf{Gewicht:}                 & ca.\ 0{,}50 kg (mit Gehäusekappe) \\ 
        \textbf{Abmaße (Ø x H):}          & 87 mm $\times$ 74{,}2 mm \\ 
        \textbf{Wellenlänge Laser:}       & 865 nm \\ 
        \textbf{Framerate:}               & 10 Hz oder 20 Hz (konfigurierbar) \\ 
        \textbf{Reichweite (80 \% Reflektivität):} 
                                          & 170 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Reichweite (10 \% Reflektivität):} 
                                          & 90 m bei $> 90\,\%$ Positivdetektion bei 100 klx Sonnenlicht \\ 
        \textbf{Fehler Distanzbestimmung:}& typ. $\pm 2{,}5$ cm (Lambert-Ziel), bis $\pm 5$ cm (retroreflektiv) \\ 
        \textbf{Horizontales FoV:}        & $360^\circ$ \\ 
        \textbf{Vertikales FoV:}          & $42{,}4^\circ \pm 1^\circ$ \\ 
        \textbf{Horizontale Auflösung:}   & 512, 1\,024 oder 2\,048 Winkelabtastungen pro Umdrehung \\ 
        \textbf{Vertikale Auflösung:}     & 128 Kanäle \\ 
        \textbf{Punktzahl pro Sekunde:}   & bis zu 5{,}24 Mio.\ Punkte/s (128 Kanäle) \\ 
        \textbf{Betriebsspannung:}        & 9{,}5 V bis 51 V DC \\ 
        \textbf{Leistungsaufnahme:}       & ca.\ 16 W (nominal, bis 28 W beim Kaltstart) \\ 
        \textbf{Datenschnittstelle:}      & UDP über Gigabit-Ethernet (1000BASE-T / 1000BASE-T1) \\ 
        \hline
    \end{tabular}
    \caption{Spezifikationen LiDAR-Sensor \emph{Ouster OS1}}
\end{table}


\subsection{Sensorpositionierung am Opel Astra}
Der im Rahmen dieser Arbeit verwendete Ouster~OS1 ist auf dem Versuchsträger Opel Astra des Instituts montiert. Der Sensor, laut \textcite{Sagdic2025}, wurde mittig auf einer Trägerstruktur über der Fahrzeugdachkante positioniert. Diese erhöhte Montage ermöglicht ein möglichst großes 360°-Sichtfeld ohne Abschattungen durch Fahrzeugkarosserie oder Anbauteile. Die Ausrichtung des Sensors gewährleistet, dass sowohl Bereiche vor dem Fahrzeug als auch seitliche und rückwärtige Zonen erfasst werden, was für die spätere Bodenfilterung, Clusterbildung und Objektverfolgung essenziell ist. Die Abbildung~\ref{fig:sensorposition_astra} zeigt die Montageposition auf dem Fahrzeug im Labor des IFZN mit Abmessungen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/sensorposition_astra.png}
    \caption{Montageposition des Ouster~OS1 auf dem Opel Astra mit Abmessungen (\textcite{Wendel2025}).}
    \label{fig:sensorposition_astra}
\end{figure}

Abbildung~\ref{fig:position_sensor} veranschaulicht den Versuchsträger Opel Astra sowie die auf der Trägerstruktur montierte Position des Ouster~OS1.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.83\textwidth]{Bilder/position_sensor.png}
    \caption{Montageposition des Ouster~OS1 auf dem Opel Astra}
    \label{fig:position_sensor}
\end{figure}

\chapter{Systemarchitektur}
\label{chap:systemarchitektur}

Dieses Kapitel beschreibt die technische Systemarchitektur der Umfelderkennung auf Basis von ROS~2 und eines Ouster~OS1. Ziel ist eine nachvollziehbare, konsistente und echtzeitfähige Verarbeitungskette. Die Darstellung umfasst (i) Grundlagen von ROS~2, (ii) Datenrepräsentation und Bibliotheken, (iii) Arbeitsumgebung, (iv) Gesamtsystem mit Datenaufnahme und Verarbeitungskette, (v) Deployment sowie (vi) Bewertungsmetriken.

In diesem Zusammenhang "structure du chapitre"

\section{Überblick über das Gesamtsystem}
\subsection{Anforderungen an den Algorithmus}

Die Punktwolken des Ouster~OS1 bilden die Grundlage einer Verarbeitungskette, die relevante Objekte im Umfeld extrahiert und nachverfolgt. Die hohe Winkelauflösung des Sensors ermöglicht eine präzise Rekonstruktion lokaler Geometrien \parencite{OusterOS1}, erfordert jedoch eine robuste Vorverarbeitung: Boden- und Störanteile müssen entfernt, die Punktdichte kontrolliert reduziert und gleichzeitig ausreichende Detailinformationen für verlässliche Bounding Boxes bewahrt werden.

Diese Schritte folgen den Empfehlungen der Fahrzeugautomatisierungs-Literatur, die deterministische Filterung und kontrollierte Latenz als Beitrag zur funktionalen Sicherheit hervorhebt \parencite{Arnold2019Survey,Macenski2022ROS2}. Eine Bodenentfernung senkt die Falsch-Positiv-Rate und verbessert die Selektivität beim Clustering \parencite{gomes2023survey}; konfigurierbare Parameter erleichtern die Adaption an unterschiedliche Szenarien wie dichten Verkehr, freie Flächen oder geneigte Fahrbahnen.

Um den Einsatz auf Standardhardware zu ermöglichen und die Reproduzierbarkeit sicherzustellen, wird eine End-to-End-Latenz unter \(100\,\text{ms}\) bei \(10\,\text{Hz}\) angestrebt, verbunden mit moderater CPU/RAM-Auslastung und klar definierten ROS~2-Nachrichten für maschinen- wie terminallesbare Ausgaben. Die Einbindung in die bestehende GUI des Projekts \emph{Carception~X} \parencite{IFZN_Projekte} stellt darüber hinaus Kompatibilität mit den vorhandenen Betriebsmodi (Sensor, Sensor + PC, Sensor + PC + Embedded PC) sicher \parencite{Wendel2025}.

Die detaillierten Anforderungen und dazugehörigen Messkriterien sind in Tabelle~\ref{tab:anforderungen_messkette} zusammengefasst.

  \begin{table}[H]
    \centering
\begin{tabularx}{\textwidth}{|c|X|X|}
\hline
\textbf{Nr.} & \textbf{Anforderung} & \textbf{Messkriterium} \\
\hline

1 &
OS1-Punktwolken werden empfangen &
Topic \texttt{/ouster/points}; Rate $\geq 10$ Hz \\[0.15cm]
\hline

2 &
Objektliste mit Pose und Abmessungen &
Topic \texttt{/tracks\_raw} (\texttt{vision\_msgs/Detection3DArray}) \\[0.15cm]
\hline

3 &
Bodenpunkte und Störsignale entfernen &
Topic \texttt{/obstacle\_points} ohne Bodenanteile \\[0.15cm]
\hline

4 &
Reduzierte Punktdichte ohne wesentlichen Geometrieverlust &
Konfigurierbare Voxelgröße; stabile FPS \\[0.15cm]
\hline

5 &
Laufzeitparameter anpassbar &
Parameter via Launch setzbar \\[0.15cm]
\hline

6 &
End-to-End-Latenz <100 ms bei 10 Hz &
Zeitstempel-Differenz Ein-/Ausgang; stabile 10 Hz \\[0.15cm]
\hline

7 &
Stabiler Betrieb auf Standardhardware &
CPU/RAM-Auslastung $\leq 60\,\%$ bei stabilen $10\,\mathrm{Hz}$ \\[0.15cm]
\hline

8 &
Maschinen- und terminallesbare Ausgabe &
Definierte ROS-Nachrichten + kompakte Konsole \\[0.15cm]
\hline

9 &
Integration in bestehende GUI &
Algorithmus ausführbar bei den Moden Sensor, Sensor + PC, Sensor + PC + Embedded PC \\[0.05cm]
\hline

\end{tabularx}
\caption{Anforderungen an die Verarbeitungsstrecke mit zugehörigen Messkriterien}
\label{tab:anforderungen_messkette}
\end{table}


\subsection{Datenerfassung}
\label{chap:datenerfassung}

Die Datenerfassung umfasst sämtliche Schritte von der Initialisierung des Ouster~OS1 über die Netzwerkkommunikation bis hin zur Bereitstellung der Punktwolken innerhalb des ROS~2-Systems. Der grundlegende Aufbau orientiert sich an der Messkette nach \cite{Wendel2025}. 

\paragraph{Netzwerkarchitektur und Übertragungspfad}
Der Ouster~OS1 wird über eine dedizierte Ethernetverbindung mit dem Embedded-PC dSPACE MAB~III verbunden. Da der Embedded-PC nur begrenzte Schnittstellen besitzt (vgl. \cite{Wendel2025}), erfolgt die physische Anbindung über einen USB-A-auf-Ethernet-Adapter des Typs \emph{Renkforce RF-4708614}. Dieser unterstützt Datenraten bis \(1\,\text{Gbit/s}\) und stellt damit sicher, dass die maximal mögliche Netto-Datenrate des LiDAR-Sensors zuverlässig übertragen werden kann (vgl. \cite{conrad2025}). Die Netzwerkkommunikation arbeitet in einem isolierten Subnetz, in dem nur Embedded-PC und Sensor verbunden sind. Dies minimiert Latenzen und verhindert Paketverlust durch konkurrierende Netzwerkdienste.

\paragraph{Publikation der Sensordaten}
Nach erfolgreicher Initialisierung publiziert der Ouster~OS1 die Daten als ROS~2-Nachrichten des Typs \texttt{sensor\_msgs/PointCloud2} auf dem Topic \texttt{/ouster/points}. Die Punktwolke enthält für jeden Messpunkt kartesische Koordinaten \((x,y,z)\), die Intensität der rückgestreuten Strahlung sowie den Ring-Index des Laserkanals. Der Zeitstempel wird direkt in der Firmware generiert und ermöglicht eine präzise Synchronisation mit späteren Verarbeitungsschritten. Durch die feste Schichtstruktur (Ringe) der OS1-Architektur bleibt die vertikale Struktur der Punktwolke konsistent, was insbesondere für die spätere Bodenextraktion und Clusterbildung essenziell ist.

\paragraph{Weiterleitung im ROS~2-Netzwerk}
Der Embedded-PC leitet die Punktwolken über das interne ROS~2-Netzwerk an den Laptop weiter. Dies erfolgt über eine DDS-basierte Publish/Subscribe-Kommunikation, deren QoS-Einstellungen so gewählt wurden, dass die hohe Datenrate des OS1 zuverlässig transportiert werden kann (vgl. \cite{Wendel2025}). Auf dem Laptop können die Daten entweder direkt in \emph{RViz2} visualisiert oder über die entwickelte GUI weiterverarbeitet werden. Die Entkopplung von Embedded-PC (Datenerfassung) und Laptop (Analyse/Visualisierung) ermöglicht eine stabile Erfassung, selbst wenn die Visualisierung oder GUI zeitweise höhere Rechenlast erzeugt.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{Bilder/messkette_sensordaten.png}
  \caption{Aufbau der Datenerfassung mit dem Ouster~OS1-LiDAR \cite{Wendel2025}).}
  \label{fig:messkette_sensordaten}
\end{figure}

\subsection{Umsetzung}
Abbildung~\ref{fig:ros2-pipeline-alltopics} zeigt die ROS~2-Verarbeitungskette von der Aufnahme über Vorverarbeitung, Bodensegmentierung und Cluster/Bounding-Boxes bis zur Objektverfolgung. Jeder Knoten hat klar definierte Ein-/Ausgänge und Topics.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=2.2cm,
    process/.style={rectangle, rounded corners, draw=black, fill=blue!5,
                    text centered, minimum width=4.2cm, minimum height=1cm},
    startstop/.style={ellipse, draw=black, fill=gray!15,
                      text centered, minimum width=3cm, minimum height=1cm},
    rviz/.style={rectangle, draw=black, fill=green!10, rounded corners,
                 text centered, minimum width=3.5cm, minimum height=0.9cm,
                 font=\scriptsize},
    arrow/.style={thick,->,>=stealth},
    every node/.style={font=\small}
  ]

  % Hauptlinie
  \node (start)   [startstop] {Start};
  \node (input)   [process, below of=start] {Messdaten einlesen};
  \node (crop)    [process, below of=input] {CropBox-Filter};
  \node (voxel)   [process, below of=crop] {VoxelGrid-Filter};
  \node (ground)  [process, below of=voxel] {Bodensegmentierung};
  \node (cluster) [process, below of=ground] {Cluster-Extraktion \& Bounding Boxes};
  \node (track)   [process, below of=cluster] {Objektverfolgung};
  \node (end)     [startstop, below of=track] {Ende};

  % RViz-Knoten (rechts)
  \node (rviz_input)  [rviz, right=4.1cm of input]  {RViz};
  \node (rviz_crop)   [rviz, right=4.1cm of crop]   {RViz};
  \node (rviz_voxel)  [rviz, right=4.1cm of voxel]  {RViz};
  \node (rviz_ground) [rviz, right=4.1cm of ground] {RViz};
  \node (rviz_cluster)[rviz, right=2.8cm of cluster]{RViz};
  \node (rviz_track)  [rviz, right=4.1cm of track]  {RViz};

  % Vertikale Pfeile (Verarbeitungskette)
  \draw[arrow] (start) -- (input);
  \draw[arrow] (input) -- node[right]{\scriptsize \texttt{/ouster/points}} (crop);
  \draw[arrow] (crop)  -- node[right]{\scriptsize \texttt{/points\_cropped}} (voxel);
  \draw[arrow] (voxel) -- node[right]{\scriptsize \texttt{/points\_voxel}} (ground);
  \draw[arrow] (ground)-- node[right]{\scriptsize \texttt{/obstacle\_points}} (cluster);
  \draw[arrow] (cluster)-- node[right]{\scriptsize \texttt{/detections\_raw}} (track);
  \draw[arrow] (track) -- node[right]{\scriptsize \texttt{/tracks\_raw}} (end);

  % Pfeile zu RViz (alle Schritte)
  \draw[arrow] (input.east)  -- node[above,sloped]{\scriptsize \texttt{/ouster/points}} (rviz_input.west);
  \draw[arrow] (crop.east)   -- node[above,sloped]{\scriptsize \texttt{/points\_cropped}} (rviz_crop.west);
  \draw[arrow] (voxel.east)  -- node[above,sloped]{\scriptsize \texttt{/points\_voxel}} (rviz_voxel.west);
  \draw[arrow] (ground.east) -- node[above,sloped]{\scriptsize \texttt{/obstacle\_points}} (rviz_ground.west);
  \draw[arrow] (cluster.east)-- node[above,sloped]{\scriptsize \texttt{/detections\_markers}} (rviz_cluster.west);
  \draw[arrow] (track.east)  -- node[above,sloped]{\scriptsize \texttt{/tracks\_markers}} (rviz_track.west);

  % Caption & Label
  \end{tikzpicture}
  \caption{ROS~2-Algorithmus mit allen Nodes, ihren Topic-Verbindungen und den Ausgaben zur Visualisierung in RViz.}
  \label{fig:ros2-pipeline-alltopics}
\end{figure}

\section{Grundlagen von ROS~2}
\label{sec:ros2_basics}
Das \emph{Robot Operating System~2 (ROS~2)} ist ein quelloffenes Framework für modulare, verteilte Robotiksysteme. Es stellt Middleware-basierte Kommunikation (DDS), wiederverwendbare Komponenten und Entwicklungswerkzeuge bereit. Dank Skalierbarkeit und Plattformunabhängigkeit wird ROS~2 in Forschung und Industrie eingesetzt (z.\,B. autonome Fahrzeuge, mobile Robotik, Inspektion).

\paragraph{Nodes}
\label{sec:ros2_nodes}

In ROS2 stellen \textit{Nodes} die grundlegenden Ausführungseinheiten des Systems dar. 
Jeder Node repräsentiert einen eigenständigen Prozess, der eine bestimmte Funktion erfüllt – beispielsweise das Erfassen von Sensordaten, die Datenverarbeitung oder die Ansteuerung von Aktoren. 
Die Modularisierung erlaubt klar definierte Schnittstellen und Wiederverwendung (vgl. \cite{ros2_docs}).

\paragraph{Topics}
\label{sec:ros2_topics}

Die Kommunikation zwischen Nodes erfolgt in ROS2 hauptsächlich über \textit{Topics}. Sie transportieren Nachrichten nach dem Publish/Subscribe-Prinzip.
Ein Node kann Daten auf einem Topic veröffentlichen (\textit{publish}) oder von diesem empfangen (\textit{subscribe}). 
Publisher und Subscriber sind entkoppelt, was flexible, verteilte Architekturen ermöglicht (vgl. \cite{ros2_docs}).

\paragraph{Nachrichten}
\label{sec:ros2_messages}

Die über Topics ausgetauschten Informationen werden in Form von \textit{Nachrichten (Messages)} übertragen. 
Eine Nachricht besteht aus einer definierten Datenstruktur, die verschiedene Datentypen wie Ganzzahlen, Gleitkommazahlen, Arrays oder benutzerdefinierte Typen enthalten kann. 
Diese klar definierte Struktur ermöglicht einen standardisierten und sicheren Datenaustausch zwischen Nodes, unabhängig von der zugrunde liegenden Programmiersprache(~\cite{ros2_docs}).

\paragraph{Bags}
\label{sec:ros2_bags}

\textit{ROS2 Bags} dienen zur Aufzeichnung, Speicherung und Wiederverwendung von Nachrichten, die über Topics ausgetauscht werden.  
Sie unterstützen Analyse, Debugging und reproduzierbare Experimente (vgl. \cite{ros2_docs}).

\paragraph{RViz2}
\label{sec:ros2_rviz2}

\textit{RViz2} ist ein Visualisierungstool, das zur Darstellung und Analyse der in ROS2 verarbeiteten Daten verwendet wird. 
Es ermöglicht die dreidimensionale Visualisierung von Punktwolken, Robotermodellen, Trajektorien und Sensordatenströmen in Echtzeit und unterstützt Entwicklung und Fehlersuche (vgl. \cite{ros2_docs}).


\paragraph{DDS, Discovery und Transports}

ROS~2 verwendet das \emph{Data Distribution Service} (DDS) als Kommunikationsmiddleware, wobei die RMW-Schicht die Kommunikation über DDS-basierte Implementierungen wie Fast~DDS und Cyclone~DDS abstrahiert. DDS übernimmt die zuverlässige Nachrichtenverteilung sowie das Teilnehmermanagement innerhalb eines ROS~2-Netzwerks \parencite{rmw_implementations}.

\begin{itemize}
  \item \textbf{Discovery:}  
  Die automatische Erkennung von Teilnehmern erfolgt über die DDS-Mechanismen SPDP und SEDP, die Multicast-Traffic zur Detektion neuer Knoten verwenden. Die \emph{Domain-ID} trennt unterschiedliche DDS-Netze logisch und bestimmt die verwendeten Ports \parencite{ros2_traffic}.
  
  \item \textbf{Transports:}  
  Standardmäßig nutzt DDS UDP für die Netzwerkommunikation. Für datenintensive Anwendungen stehen jedoch optimierte Transportpfade wie Shared-Memory und intra-process-Kommunikation zur Verfügung, die Kopien reduzieren und Latenzen minimieren \parencite{fastdds_shm}.
  
  \item \textbf{Konfiguration:}  
  DDS-Einstellungen wie genutzte Netzwerkschnittstellen, Whitelists, Zeitlimits oder Transportparameter können über XML-Profile oder Umgebungsvariablen konfiguriert werden. Cyclone DDS lädt Konfigurationen über die Variable \texttt{CYCLONEDDS\_URI} \parencite{cyclonedds_config}.
\end{itemize}

Praktisch empfiehlt es sich, bei Systemen mit hoher Punktdichte (z.\,B.\ LiDAR-Verarbeitung) Shared-Memory- oder intra-process-Kommunikation zu aktivieren und die Netzwerkschnittstelle explizit zu wählen, um Discovery-Traffic zu reduzieren und eine stabile Datenübertragung sicherzustellen \parencite{fastdds_shm}.

\paragraph{Quality of Service (QoS)}
\label{sec:ros2_qos}
In ROS~2 definieren Quality-of-Service-Profile (QoS) die Kommunikationssemantik zwischen Publishern und Subscribern. Sie beeinflussen maßgeblich Latenz, Paketverluste, Speicherbedarf und Synchronisation entlang der Verarbeitungspipeline. Besonders bei hochfrequenten Sensorströmen wie LiDAR ist eine konsistente und angepasste QoS-Konfiguration entscheidend für die Systemstabilität.

Die Konfiguration der QoS-Profile in ROS~2 umfasst mehrere Parameter, von denen insbesondere \textbf{Reliability}, \textbf{History/Depth} und \textbf{Durability} die Übertragungseigenschaften von Sensordaten maßgeblich bestimmen. Eine kompakte Übersicht der Vor- und Nachteile findet sich in Tab.~\ref{tab:qos_vt_nt}. Die Definitionen basieren auf den DDS-Spezifikationen gemäß OMG-Standard \parencite{OMG_DDS_Spec}.

\begin{table}[H]
\centering
\caption{Wesentliche QoS-Parameter und ihre Vor- und Nachteile}
\label{tab:qos_vt_nt}
\begin{tabularx}{\textwidth}{l X X}
\toprule
\textbf{Parameter} & \textbf{Vorteile} & \textbf{Nachteile} \\
\midrule
\textbf{Reliability: Reliable} & Garantierte Zustellung; keine Datenverluste & Höhere Latenz bei hoher Last; höherer Ressourcenbedarf \\
\textbf{Reliability: BestEffort} & Niedrige Latenz; ressourcenschonend & Nachrichten können verloren gehen \\
\textbf{History: KeepLast(N)} & Begrenzter Speicherverbrauch; stabil vorhersehbar & Ältere Nachrichten werden verworfen \\
\textbf{History: KeepAll} & Keine Nachricht geht verloren & Hoher Speicherbedarf; ungeeignet bei hohen Datenraten \\
\textbf{Durability: Volatile} & Ideal für Echtzeitströme; keine Verzögerung & Späte Subscriber erhalten keine alten Daten \\
\textbf{Durability: TransientLocal} & Neue Subscriber erhalten letzte Nachricht sofort & Erhöhter Speicheraufwand; nicht geeignet für LiDAR-Daten \\
\bottomrule
\end{tabularx}
\end{table}

Die \textbf{Reliability-Policy} legt fest, wie mit Nachrichtenverlusten umgegangen wird. Bei \emph{Reliable} garantiert DDS die Zustellung aller Pakete, was für deterministische Verarbeitungsschritte erforderlich ist, jedoch bei hohen Sensordatenraten zu Latenzsteigerungen führen kann. Dies entspricht dem von ROS~2 empfohlenen Verhalten für kritische Datenströme \parencite{Maruyama2016ROS2}. \emph{BestEffort} verzichtet dagegen auf Garantien und ist insbesondere für Visualisierungstopics wie \texttt{/ouster/points} geeignet, da dort gelegentliche Verluste tolerierbar sind.

Die \textbf{History-Einstellung} bestimmt, wie viele Nachrichten gepuffert werden. Für LiDAR-Anwendungen mit hohen Datenraten empfiehlt sich \emph{KeepLast} mit geringer Tiefe (5--10), da kontinuierlich neue Punktwolken eintreffen und ältere Daten obsolet werden. Diese Empfehlung findet sich ebenfalls in ROS~2-Profilingstudien, die den hohen Speicherbedarf von \emph{KeepAll} dokumentieren \parencite{Buhlmann2022QoSStudy}.

Die \textbf{Durability} sollte bei Echtzeitdaten grundsätzlich \emph{Volatile} sein, da nur aktuelle Sensordaten relevant sind. \emph{TransientLocal} eignet sich hingegen für selten publizierte Konfigurations- oder Statusnachrichten, für die neue Subscriber die zuletzt bekannte Information sofort erhalten sollen \parencite{DDS_Primer2018}.

Weitere QoS-Parameter wie \emph{Deadline}, \emph{Liveliness} und \emph{Lifespan} spielen im LiDAR-Kontext eine untergeordnete Rolle. Sie können jedoch zur Laufzeitüberwachung eingesetzt werden, beispielsweise zur Erkennung ausgefallener Publisher oder verzögerter Messungen \parencite{ROS2_Design_QoS}.


In der Praxis ergeben sich daraus folgende empfohlenen Einstellungen: 
Für die Verarbeitungskette (z.\,B. \texttt{/points\_cropped}, \texttt{/detections\_raw}) 
ist ein Profil mit \emph{Reliable}, \emph{KeepLast(10)} und \emph{Volatile} zweckmäßig, 
um Datenverluste bei gleichzeitig moderater Pufferung zu vermeiden. 
Für Visualisierungstopics wie \texttt{/ouster/points} genügt meist \emph{BestEffort} 
mit geringer Tiefe, da verpasste Nachrichten tolerierbar sind.

Die Tabelle ~\ref{tab:qos-topics} zeigt die QoS-Profile der in dieser Arbeit erstellten Topics. Publisher und Subscriber handeln bei Verbindungsaufbau die Schnittmenge ihrer QoS-Profile aus. Dabei können Fehlermodi auftreten: etwa wenn ein \emph{Reliable}-Publisher mit einem \emph{BestEffort}-Subscriber verbunden wird – in diesem Fall wird die Kommunikation zwar aufgebaut, aber der Publisher puffert gegebenenfalls unkontrolliert. Ebenso führt ein Mismatch bei der \emph{Durability} dazu, dass historische Nachrichten nicht übertragen werden. Eine zu geringe \emph{Depth} wiederum kann zu Paketverlusten oder wachsender Latenz führen. Daher sollten QoS-Parameter explizit je Topic gesetzt und aktiv überwacht werden.

\begin{table}[htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}

\begin{tabularx}{\textwidth}{|l|c|l|l|X|}
\hline
\textbf{Knoten} & \textbf{Rolle} & \textbf{Topic} & \textbf{Typ} & \textbf{QoS} \\
\hline

crop\_box\_node & Sub & \texttt{/ouster/points} & PointCloud2       & SensorDataQoS (Best Effort) \\ \hline
crop\_box\_node & Pub & \texttt{/points\_cropped} & PointCloud2     & SensorDataQoS (Best Effort) \\ \hline

voxel\_filter\_node & Sub & \texttt{/points\_cropped} & PointCloud2    & SensorDataQoS (Best Effort) \\ \hline
voxel\_filter\_node & Pub & \texttt{/points\_voxel} & PointCloud2      & SensorDataQoS (Reliable) \\ \hline

ransac\_ground\_node & Sub & \texttt{/points\_voxel} & PointCloud2      & SensorDataQoS (Best Effort) \\ \hline
ransac\_ground\_node & Pub & \texttt{/obstacle\_points} & PointCloud2   & rclcpp::QoS(10) (Reliable) \\ \hline

cluster\_extraction\_node & Sub & \texttt{/obstacle\_points} & PointCloud2  & SensorDataQoS (Best Effort) \\ \hline
cluster\_extraction\_node & Pub & \texttt{/detections\_markers} & MarkerArray & rclcpp::QoS(10) (Reliable) \\ \hline
cluster\_extraction\_node & Pub & \texttt{/detections\_raw} & Detection3DArray & SensorDataQoS (Best Effort) \\ \hline

sort\_tracker\_node & Sub & \texttt{/detections\_raw} & Detection3DArray & SensorDataQoS (Best Effort) \\ \hline
sort\_tracker\_node & Pub & \texttt{/tracks\_raw} & Detection3DArray  & rclcpp::QoS(10) (Reliable) \\ \hline
sort\_tracker\_node & Pub & \texttt{/tracks\_markers} & MarkerArray     & rclcpp::QoS(10) (Reliable) \\ \hline

\end{tabularx}

\caption{QoS-Profile der im Code verwendeten Topics (Default-Werte).}
\label{tab:qos-topics}
\end{table}

\section{Datenrepräsentation und Bibliotheken}
\subsection{\texttt{sensor\_msgs/PointCloud2}: Felder, Koordinaten und Layout}
Ouster~OS1-Daten werden als \texttt{sensor\_msgs/PointCloud2} publiziert. Tabelle~\ref{tab:pointcloud2-fields} fasst die wichtigsten Felder und deren Semantik zusammen; die konkreten Datentypen (z.\,B. \texttt{float32} oder \texttt{uint16}) hängen von Treiber- und Konfiguration ab.

\begin{table}[H]
  \centering
  \small % réduit la taille
  \setlength{\tabcolsep}{6pt} % réduit l'espace horizontal
  \renewcommand{\arraystretch}{1.05} % compacte verticalement

  \begin{tabular}{|p{2.2cm}|p{13cm}|}
    \hline
    \textbf{Feld} & \textbf{Bedeutung} \\ \hline

    \texttt{x, y, z} &
    Kartesische Koordinaten (m) im \texttt{frame\_id}. 
    Rechtsdrehendes System nach REP~103: \(x\) vorwärts, \(y\) links, \(z\) oben. \\ \hline

    \texttt{intensity} &
    Reflektierte Signalstärke (\texttt{uint16}); abhängig von Sensorkalibrierung und daher nur innerhalb eines Scans vergleichbar. \\ \hline

    \texttt{ring} &
    Vertikaler Kanalindex (Laser/Detektor). Beim OS1-64: 64 Ringe von 0 (unten) bis 63 (oben). \\ \hline

    \texttt{time} &
    Zeitversatz eines Punktes relativ zu \texttt{header.stamp} (s); wichtig für Motion Compensation innerhalb eines Scans. \\ \hline

  \end{tabular}

  \caption{Zentrale Felder von \texttt{sensor\_msgs/PointCloud2} für den Ouster~OS1.}
  \label{tab:pointcloud2-fields}
\end{table}


Alle Felder werden typischerweise little-endian kodiert (\texttt{is\_bigendian = false}). Das Speicherlayout folgt einem linearen Puffer \texttt{data} mit \texttt{point\_step} (Byte pro Punkt) und \texttt{row\_step} (Byte pro Zeile). Die Dimensionen \texttt{width}/\texttt{height} beschreiben eine 2D-Anordnung: Bei rotierenden LiDARs entspricht \texttt{height} der Ringanzahl, \texttt{width} der Punktzahl pro Umdrehung. \texttt{is\_dense} signalisiert, ob \enquote{NaN}-Einträge vorkommen. Eine organisierte Wolke (Ring~\(\times\) Azimut) ermöglicht effiziente nachbarschaftsbasierte Filter und Segmentierung sowie ringweise Verarbeitung (z.\,B. Bodenfilter nur auf unteren Ringen).

\subsection{Point Cloud Library (PCL)}
\label{chap:pcl}
Die \emph{Point Cloud Library (PCL)} bildet die Grundlage der Umfelderkennung. Sie ist eine freie C++-Bibliothek (BSD-Lizenz), die ursprünglich 2011 bei Willow~Garage als Teil des ROS-Ökosystems entstand und seitdem von der Community rund um Open~Perception und industrielle Anwender weiterentwickelt wird (vgl. \cite{pcl_docs_2025}). PCL bietet eine umfangreiche Sammlung modularer Algorithmen: Punktfilterung (Downsampling, Zuschneiden, Rauschunterdrückung), Normalen- und Merkmalsextraktion, RANSAC-basierte Segmentierung, Registrierung (z.\,B. ICP), Oberflächenrekonstruktion, Nachbarschaftssuchen auf k-d-Baum/Octree-Basis, Clusteranalyse sowie Visualisierung und Dateiformate (PCD). In ROS~2 erfolgt die Einbindung über \texttt{pcl\_conversions} und ggf. \texttt{pcl\_ros}; \texttt{sensor\_msgs/PointCloud2} wird bidirektional in \texttt{pcl::PointCloud<T>} konvertiert.

\subsection{Genutzte Module und Algorithmen}
Die Verarbeitungskette stützt sich auf etablierte PCL-Module, die in ROS~2 stabil verfügbar sind und nachvollziehbare Laufzeiteigenschaften besitzen:
\begin{itemize}
  \item \textbf{Filter}: \texttt{CropBox}/\texttt{PassThrough} begrenzen den Arbeitsbereich auf das relevante Fahrumfeld und reduzieren die Punktzahl frühzeitig (vgl. Abschnitt~\ref{sec:cropbox}); \texttt{VoxelGrid} homogenisiert die Punktdichte und senkt den Rechenaufwand nachgelagerter Schritte ohne signifikanten Geometrieverlust (Abschnitt~\ref{sec:voxelgrid}). Auf zusätzliche Ausreißerfilter (z.\,B. \texttt{StatisticalOutlierRemoval}) wird in der implementierten Verarbeitungskette zugunsten geringerer Latenz verzichtet; das Rohsignal erwies sich im Testfeld als hinreichend stabil (Kapitel~\ref{chap:vorverarbeitung}).
  \item \textbf{Segmentierung}: Eine plane RANSAC-Segmentierung trennt Boden- von Hindernispunkten. \texttt{pcl::SACSegmentation} mit \texttt{SACMODEL_PLANE} ist robust gegenüber Ausreißern, erfordert nur wenige Parameter (Abstands- und Iterationsschranke) und lässt sich effizient in Echtzeit ausführen. Die methodische Begründung und Parametertabellen finden sich in Kapitel~\ref{chap:bodensegmentierung}.
  \item \textbf{Clustering}: \texttt{EuclideanClusterExtraction} auf Basis eines \texttt{pcl::search::KdTree} gruppiert die bodenfreien Punkte. Der Ansatz liefert reproduzierbare Ergebnisse, benötigt nur \(\varepsilon\)-Nachbarschaft und Min/Max-Clustergröße als Stellgrößen und ist in zahlreichen LiDAR-Verarbeitungsketten erprobt. Die konkrete Umsetzung und Parameterstudie stehen in Kapitel~\ref{chap:clustering}.
  \item \textbf{Konvertierung}: \texttt{pcl::fromROSMsg}/\texttt{pcl::toROSMsg} sowie \texttt{pcl_conversions} bilden die Brücke zwischen ROS~2-Nachrichten und PCL-Datenstrukturen. Sie sind erforderlich, damit die oben genannten PCL-Algorithmen in den ROS-Knoten der Vorverarbeitung, Bodensegmentierung und Clusterung arbeiten können.
\end{itemize}
\textbf{Performance}: Durch Vorallokation (z.\,B. Puffergrößen für KdTree und Clusterlisten), die Vermeidung redundanter Kopien und die Wahl leichter Punkttypen wie \texttt{pcl::PointXYZI} wird eine konstante Verarbeitung bei 10~Hz ermöglicht; die Wirkung der Einstellungen wird in den Kapitelabschnitten zu Vorverarbeitung (Kapitel~\ref{chap:vorverarbeitung}) und Clustering (Kapitel~\ref{chap:clustering}) gemessen und diskutiert.

Die genannten Module tauchen entlang der gesamten Verarbeitungskette wieder auf: Die Filterstufe in Kapitel~\ref{chap:vorverarbeitung} bereitet die Daten für die RANSAC-Bodensegmentierung in Kapitel~\ref{chap:bodensegmentierung} vor; deren Ergebnis dient als Ausgangspunkt für die Cluster-Extraktion und Bounding-Box-Ermittlung in Kapitel~\ref{chap:clustering}. Dadurch ist nachvollziehbar, warum jede Komponente ausgewählt wurde und wie sie mit den nachfolgenden Modulen interagiert.
Für Entwicklung und Ausführung wird \textbf{Ubuntu~22.04~LTS (Jammy Jellyfish)} verwendet. Als Referenzplattform für ROS~2 ermöglicht Ubuntu eine nahtlose Integration von Bibliotheken, Treibern und Werkzeugen (vgl. \cite{ubuntu_docs_2025}). Die Distribution bietet stabile C++/Python-Toolchains, hohe Sicherheit und breite Unterstützung in wissenschaftlicher wie industrieller Softwareentwicklung. Die enge Verzahnung mit der in dieser Arbeit eingesetzten ROS-Distribution \emph{Humble~Hawksbill} vereinfacht die Einrichtung der Abhängigkeiten. Die aktive Entwicklergemeinschaft sorgt durch regelmäßige Updates, umfassende Dokumentation und eine große Auswahl an Open-Source-Paketen für einen reibungslosen Entwicklungsprozess. Dank der modularen Struktur von Linux lässt sich die Arbeitsumgebung flexibel an die spezifischen Anforderungen der Sensorintegration und der ROS-Module anpassen.

\subsection{Bash-Skripte}
Bash-Skripte automatisieren wiederkehrende Aufgaben (Workspace laden, Launch starten, Aufzeichnung) und sind ein zentrales Werkzeug in Linux-basierten Entwicklungsumgebungen. In dieser Arbeit fungiert ein Skript als Bindeglied zwischen der GUI und dem Python-\texttt{launch}-File der Verarbeitungskette. Der typische Ablauf umfasst folgende Schritte:
\begin{enumerate}
  \item Laden der ROS~2-Umgebung (\texttt{source /opt/ros/humble/setup.bash}) und des projektinternen Workspace (\texttt{source \$\{WORKSPACE\}/install/setup.bash}) über \texttt{set -euo pipefail}, damit Fehler frühzeitig abbrechen.
  \item Übergabe benutzerdefinierter Parameter (z.\,B. Topic-Namen, Ausgabeordner, Logging-Level) aus der GUI via Shell-Argumente; diese werden an das Python-Launch-File durchgereicht.
  \item Start des zentralen Launch-Files (\texttt{ros2 launch...}) als Hintergrundprozess
  \item Persistente Ablage von Logs und Bags in einer datierten Ordnerstruktur (z.\,B. \texttt{logs/\$\{DATE\}}) sowie Ausgabe standardisierter Statusmeldungen (\texttt{echo}, \texttt{printf}).
\end{enumerate}
  Dadurch werden alle erforderlichen Knoten in definierter Reihenfolge aktiviert, die GUI entlastet und ein reproduzierbarer Start der Verarbeitungskette ermöglicht (inklusive optionaler Aufzeichnung und definierter Bereinigung bei Abbruch). Der Ablauf orientiert sich an den offiziellen ROS~2-Empfehlungen für Launch-Dateien und Bash-Wrapper-Skripte\parencite{ros2_launch_docs}.

%\subsection*{ROS~2 CLI und Monitoring}
%Zur Inspektion der Laufzeitumgebung werden \texttt{ros2}-Werkzeuge genutzt: \texttt{topic list/info/hz/bw/delay}, \texttt{node list/info}, \texttt{interface show}, \texttt{doctor}. Sie unterstützen Durchsatz-/Latenzschätzung, Interface-Inspektion und Grunddiagnostik.

\section{Deployment und Packaging}
Die Umsetzung erfolgt im ROS~2-Workspace (\texttt{colcon}); \texttt{src/} enthält die Pakete, \texttt{build/}/\texttt{install/}/\texttt{log/} werden erzeugt. Abhängigkeiten werden in \texttt{package.xml} deklariert und in \texttt{CMakeLists.txt} gebunden. Eine Trennung in Bibliotheken (Kernlogik) und Executables (ROS~2-Anbindung) erhöht die Wiederverwendbarkeit.

Start und Parametrisierung erfolgen über Launch-Dateien. Namensräume strukturieren Topics (z.\,B. Sensorpfad, Verarbeitungspfad) und erlauben parallele Instanziierung. Die Visualisierung erfolgt über \textbf{RViz2}.

\section{Leistungsmetriken und Verifikation}

Die im Folgenden betrachteten Metriken sind kein Selbstzweck, sondern direkt aus der
Anforderungsdefinition und dem Stand der Technik abgeleitet. Veröffentlichte Arbeiten zu
Echtzeit-Objekterkennung evaluieren systematisch Ende-zu-Ende-Latenz sowie CPU/RAM-Bedarf
als zentrale Gütekriterien (vgl. \cite{feng2021survey}). Genau diese Größen entsprechen den in
Tabelle~\ref{tab:anforderungen_messkette} formulierten Messkriterien: \textbf{Anforderung~6}
fordert eine Ende-zu-Ende-Latenz von unter \(100\,\text{ms}\) bei stabilen \(10\,\text{Hz}\),
\textbf{Anforderung~7} begrenzt die Auslastung auf \(\leq 60\,\%\) CPU/RAM auf Standardhardware.
Die gewählten Verifikationsschritte (Zeitstempel-Instrumentierung für Latenz, Profiler für
Ressourcennutzung) knüpfen somit unmittelbar an die definierten Anforderungen an und machen
nachvollziehbar, wie deren Erfüllung gemessen und nachgewiesen wird.

\subsection{E2E-Latenz}
Für das betrachtete System wird eine Ende-zu-Ende-Latenz von weniger als \(100\,\text{ms}\) bei einer Verarbeitung mit \(10\,\text{Hz}\) angestrebt. Zur Verifikation werden an den Modulgrenzen Zeitstempel gesetzt (Eingang und Ausgang), die anschließend hinsichtlich Mittelwert, 95. Perzentil und 99. Perzentil über lange Sequenzen ausgewertet werden. Die Messungen erfolgen bei der Wiedergabe identischer Datensätze aus \texttt{rosbag2}. Die Instrumentierung erfolgt anhand des Eingangszeitstempels aus \texttt{header.stamp}, während Zwischenzeiten innerhalb der Module mitgeführt und protokolliert werden. Eine gemeinsame Zeitbasis (z.\,B. deaktivierte Simulationszeit oder externe Synchronisation) wird vorausgesetzt, um Vergleichbarkeit und Konsistenz zu gewährleisten.

\subsection{Ressourcennutzung (CPU/RAM)}
Die Auslastung der Rechen- und Speicherressourcen wird für jeden Knoten unter repräsentativer Last erfasst. Optimierungsmaßnahmen wie ein angepasstes Datenlayout, die Vermeidung unnötiger Kopien oder die Vorallokation von Puffern werden dokumentiert und quantitativ bewertet. Zur Analyse kommen Werkzeuge wie \texttt{ros2 topic hz}, \texttt{ros2 topic bw}, \texttt{ros2 topic delay} sowie Systemprofiler (\texttt{top}/\texttt{htop}) zum Einsatz. Für Variantenvergleiche werden die Ergebnisse in Form persistenter Artefakte (z.\,B. CSV- oder JSON-Dateien) abgelegt.

\chapter{Vorverarbeitung}
\label{chap:vorverarbeitung}
\section{CropBox-Filter}
\label{sec:cropbox}

\subsection{Prinzip}
Der \textit{CropBox-Filter} begrenzt eine Punktwolke auf eine achsenparallel ausgerichtete Begrenzungsbox (Axis-Aligned Bounding Box, AABB) und entfernt sämtliche Punkte außerhalb des Volumens. Formal sei \(\mathcal{P}=\{\mathbf{p}_i\}\) eine Punktwolke mit \(\mathbf{p}_i=[x_i,y_i,z_i,1]^\top\) in homogenen Koordinaten. Der Filter prüft für jede Stichprobe, ob
\begin{align}
\mathbf{b}_\text{min} \leq \mathbf{R}\,\mathbf{p}_i + \mathbf{t} \leq \mathbf{b}_\text{max}
\label{eq:cropbox}
\end{align}
gilt, wobei \(\mathbf{R}\in\mathbb{R}^{3\times4}\) die ersten drei Zeilen der PCL-internen Transformationsmatrix sind, \(\mathbf{t}\) eine optionale Translation darstellt und \(\mathbf{b}_\text{min/max}=[x_{\min/\max},y_{\min/\max},z_{\min/\max}]^\top\) die parametrisierte Box definieren. Punkte, die diese Ungleichung nicht erfüllen, werden verworfen. Die Operation besitzt eine lineare Laufzeit \(\mathcal{O}(|\mathcal{P}|)\) und reduziert das Datenvolumen vor nachgelagerten Schritten signifikant (vgl. \cite{rusu2011pcl,pcl_cropbox_2025}). Durch die optionale Transformationsmatrix lassen sich zusätzlich gekippte Sensorkoordinaten oder Fahrzeugversätze berücksichtigen, wodurch die AABB effektiv einer beliebig orientierten Bounding Box im globalen Rahmen entspricht \cite{pcl_docs_2025}.

Die Auswahl der Grenzen folgt in dieser Arbeit einem systematischen Verfahren: Zunächst wurde eine großzügige Box \(\mathbf{b}_\text{min}=[-20,-15,-3]^\top\), \(\mathbf{b}_\text{max}=[20,15,5]^\top\) definiert, um alle relevanten Rückläufer sicher zu erfassen. Anschließend erfolgte eine schrittweise Schrumpfung entlang \(x\), \(y\) und \(z\) in \(0{,}5\,\mathrm{m}\)-Schritten. Bewertet wurde jede Konfiguration anhand (i) Detektionspräzision nach dem Clustering (Anteil falsch positiver Objekte), (ii) verbleibender Punktzahl sowie (iii) Anteil abgeschnittener Fahrbahnränder in RViz. Dieses Vorgehen entspricht der iterativen ROI-Abgrenzung, wie sie in LiDAR-Pipelines zur Falsch-Alarm-Reduktion empfohlen wird \cite{hu2013robust,himmelsbach2010fast}. Die finalen Grenzen \(\mathbf{b}_\text{min}=[-10,-6,-3]^\top\) und \(\mathbf{b}_\text{max}=[10,6,5]^\top\) lieferten den besten Kompromiss aus reduzierter Punktzahl und stabiler Objektabdeckung im urbanen Messfeld.

\subsection{Implementierung}
Im Rahmen dieser Arbeit wurde der \texttt{crop\_box\_node} in C++ unter ROS~2
entwickelt, um diesen Filter dynamisch innerhalb der gesamten Verarbeitungskette anzuwenden.
Der Node empfängt eine eingehende Punktwolke vom Ouster-LiDAR-Sensor auf dem Topic
\texttt{/ouster/points}, filtert die Daten gemäß den eingestellten Grenzen und publiziert die
gefilterte Punktwolke auf \texttt{/points\_cropped}.
Die Parameter \texttt{min\_bound} und \texttt{max\_bound} definieren die minimale und maximale Ausdehnung
des Arbeitsvolumens in Metern, während die Parameter \texttt{input\_topic} und
\texttt{output\_topic} den Ein- und Ausgang der Datenströme steuern.
Das Konzept ist in Tabelle~\ref{tab:cropbox_params} zusammengefasst.
Der produktive Code des Nodes liegt unter \texttt{ouster\_cpp/src/crop\_box\_node.cpp} und ist
in Anhang~\ref{lst:cropbox_node} vollständig abgedruckt.
Der \texttt{crop\_box\_node} wurde in C++ unter ROS~2 implementiert und nutzt die PCL-Schnittstellen, um die Ungleichung aus Gleichung~\eqref{eq:cropbox} Punkt für Punkt anzuwenden. Wichtige Kernschritte sind: Laden der Parameter \texttt{min\_bound} und \texttt{max\_bound}, Subscription des Eingangstopics \texttt{/ouster/points} mit SensorDataQoS und Publikation des Ergebnisses auf \texttt{/points\_cropped}. Ein Auszug zeigt den wesentlichen Ablauf \cite{pcl_docs_2025}:

\begin{verbatim}
// Ausschnitt aus src/ouster_cpp/src/crop_box_node.cpp
const float xmin = min_bound_[0], xmax = max_bound_[0];
const float ymin = min_bound_[1], ymax = max_bound_[1];
const float zmin = min_bound_[2], zmax = max_bound_[2];

for (; ix != ix.end(); ++ix, ++iy, ++iz) {
  const float x = *ix, y = *iy, z = *iz;
  // AABB-Test
  if (x >= xmin && x <= xmax &&
      y >= ymin && y <= ymax &&
      z >= zmin && z <= zmax) {
    // Punkt in Ausgabepuffer übernehmen
  }
}
\end{verbatim}

Neben den Pflichtfeldern \(x,y,z\) unterstützt der Node optionale Felder (\texttt{intensity}, \texttt{ring}) und erlaubt über die Callback-API \texttt{add\_on\_set\_parameters\_callback} eine Laufzeitänderung der Grenzen ohne Neustart des Nodes. Tabelle~\ref{tab:cropbox_params} fasst die Startparameter zusammen.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{/ouster/points} \\ \hline
    \texttt{output\_topic} & \texttt{/points\_cropped} \\ \hline
    \texttt{min\_bound}    & \([-20.0, -15.0, -3.0]\) \\ \hline
    \texttt{max\_bound}    & \([20.0, 15.0, 5.0]\) \\ \hline
  \end{tabular}
  \caption{Startparameter des CropBox-Filters}
  \label{tab:cropbox_params}
\end{table}

Das Zuschneidevolumen bezieht sich auf das Sensorkoordinatensystem des Ouster-LiDARs
und folgt der in ROS etablierten Konvention nach REP~103: \(x\) zeigt in Fahrtrichtung,
\(y\) nach links und \(z\) nach oben; der Ursprung liegt im LiDAR-Frame auf Fahrzeughöhe.
Die Vektoren \texttt{min\_bound} und \texttt{max\_bound} geben damit den minimalen bzw.
maximalen Wert entlang dieser Achsen in Metern an und definieren eine achsenparallele
Box, innerhalb derer Punkte beibehalten werden.

Der \texttt{crop\_box\_node} wurde so implementiert, dass er sowohl Pflichtfelder 
(\texttt{x}, \texttt{y}, \texttt{z}) als auch optionale Felder wie 
\texttt{intensity} (FLOAT32/UINT16) und \texttt{ring} erkennt.  
Für jeden Punkt wird geprüft, ob er innerhalb des durch die Parameter definierten 
Volumens liegt. Ist dies der Fall, wird er beibehalten und in die 
Ausgabepunktwolke übernommen. 
Die Implementierung erlaubt zudem eine Laufzeitänderung der Grenzen
durch dynamische Re-Konfiguration via \texttt{set\_parameters}, ohne dass
die ROS~2-Verbindungen neu aufgebaut werden müssen.

Da sich im Messbereich Gebäude und weitere störende Objekte befanden, welche die Anzahl falscher Detektionen signifikant erhöhen
 konnten, wurden die Grenzen des \texttt{crop\_box\_node} sukzessive angepasst. Durch iterative Parameteroptimierung ergaben sich
 folgende final verwendete Werte: \texttt{min\_bound = [-10.0, -6.0, -3.0]} sowie \texttt{max\_bound = [10.0, 6.0, 5.0]}.

\subsection{Test und Ergebnis}
Die Wirkung des CropBox-Filters wird nicht allein über Visualisierungen überprüft, sondern durch eine
schrittweise Verifikation. Dafür wird das aufgezeichnete ROS~2-Bag einer T-Kreuzung auf ebenem Asphaltboden
(bewusst gewählt, um symmetrische Sichtfelder und eine klare Bodenreferenz zu erhalten) mehrfach wiedergegeben.
Pro Wiederholung werden drei Messgrößen protokolliert: (1) die Zahl der Eingangspunkte, (2) die Zahl der
beibehaltenen Punkte innerhalb der AABB sowie (3) die zeitliche Stempelverschiebung zwischen Eingangs- und
Ausgangsnachricht als Indikator der Knotendauer. Ein Skript fasst die Ergebnisse als CSV zusammen und
ermöglicht den Vergleich verschiedener Parameterkombinationen.

Das Testszenario „Kreuzung auf ebenem Boden“ ist gewählt, weil es sowohl freie Sichtflächen als auch klar
begrenzte Hindernisse (Randbebauung, Bordsteine) bietet und damit typische urbanen Einsatzbedingungen
repräsentiert, ohne dass Bodenunebenheiten die Filterbewertung verfälschen. In der finalen Konfiguration
reduziert der Filter die Punktanzahl reproduzierbar um rund zwei Drittel, während der Zeitstempel im Header
(vgl. Listing~\ref{lst:cropbox_node}, Zeilen 90ff.) sicherstellt, dass nachgelagerte Latenzmessungen den korrekten
Sendezeitpunkt erhalten.
Da sich im Messbereich Gebäude und weitere störende Objekte befanden, welche die Anzahl falscher Detektionen signifikant erhöhen konnten, wurden die Grenzen des \texttt{crop\_box\_node} sukzessive angepasst. Durch iterative Parameteroptimierung ergaben sich folgende final verwendete Werte: \texttt{min\_bound = [-10.0, -6.0, -3.0]} sowie \texttt{max\_bound = [10.0, 6.0, 5.0]}.
Physikalisch orientieren sich diese Grenzen an der Testumgebung: Mit \(\pm10\,\text{m}\)
in \(x\)-Richtung wird der relevante Bereich vor und hinter dem Fahrzeug für Manöver im
innerstädtischen Geschwindigkeitsbereich erfasst, ohne Gebäude an weit entfernten
Fassaden mitzunehmen. \(\pm6\,\text{m}\) in \(y\)-Richtung decken eine Fahrspur sowie
Gegen- oder Parkspur ab (typisch \(3{-}3{,}5\,\text{m}\) je Spur) und schließen Gehwege
ein, während die Vertikalgrenzen von \(-3\,\text{m}\) bis \(5\,\text{m}\) sowohl den
Fahrbahnbereich unterhalb des Sensors als auch überhängende Objekte wie Lkw-Aufbauten
oder niedrige Brücken berücksichtigen.

Abbildung~\ref{fig:cropbox_compare} zeigt die Wirkung des Filters im Vergleich:
Links ist die ursprüngliche Punktwolke mit vollständiger Umgebung dargestellt,
während rechts nur der relevante Bereich nach Anwendung des CropBox-Filters zu sehen ist. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/cropbox_compare.png}
  \caption{Vergleich der Punktwolke vor (links) und nach (rechts) Anwendung des 
  \textit{CropBox-Filters}.}
  \label{fig:cropbox_compare}
\end{figure}


\section{VoxelGrid-Filter}
\label{sec:voxelgrid}

\subsection{Prinzip}
Der \textit{VoxelGrid-Filter} reduziert die Punktanzahl, indem der Raum in kubische Voxel der
Kantenlänge $l=\texttt{voxel\_size}$ diskretisiert wird und pro Voxel nur ein repräsentativer Punkt
(z.\,B. der erste oder ein Schwerpunkt) beibehalten wird. Dadurch sinken Datenrate und
Rechenaufwand nachgelagerter Schritte (Bodenebensegmentierung, Clustering, Tracking), bei
gleichzeitiger Kontrolle des Informationsverlustes über die Wahl von $l$.
Mathematisch erfolgt zunächst eine Quantisierung der Punktkoordinaten $(x,y,z)$ auf ganzzahlige
Gitterindizes $(i,j,k)$ über
\begin{equation}
  (i,j,k) = \left(\left\lfloor \tfrac{x}{l} \right\rfloor, \left\lfloor \tfrac{y}{l} \right\rfloor, \left\lfloor \tfrac{z}{l} \right\rfloor\right),
  \label{eq:voxel_index}
\end{equation}
wodurch alle Punkte innerhalb eines Voxels dieselben Indizes teilen. Für jedes belegte Voxel wird
genau ein Repräsentant behalten. In der vorliegenden Implementierung ist das der erste verarbeitete
Punkt pro Voxel; alternativ könnte der Schwerpunkt
\begin{equation}
  \bar{p}_v = \tfrac{1}{|V|} \sum_{p \in V} p
  \label{eq:voxel_centroid}
\end{equation}
als repräsentativer Punkt gewählt werden. Beide Varianten begrenzen die Anzahl der verbleibenden
Punkte auf höchstens ein Sample pro Voxel und machen den Aufwand proportional zur Anzahl belegter
Voxel. Die Wahl der Voxelkante $l$ steuert damit direkt den Kompromiss zwischen Detailtreue und
Rechenaufwand.
Das Funktionsprinzip entspricht der gängigen Downsampling-Strategie in der Literatur
(vgl. Abbildung~\ref{fig:voxel_principle}), siehe z.\,B. die Darstellung in \emph{Applied Sciences}
(2024)\footnote{Abbildung nach Quelle: \url{https://www.mdpi.com/2076-3417/14/8/3160}.}.

% préambule : \usepackage{graphicx}
\begin{figure}[H]
  \centering
  \includegraphics[width=.82\textwidth]{Bilder/voxelgrid_prinzip.png}%
  \caption{Funktionsprinzip des VoxelGrid-Filters: Aufteilung des Raums in Voxel und
  Beibehaltung eines repräsentativen Punktes pro Voxel (nach Lyu~u.\,a., 2024).}
  \label{fig:voxel_principle}
\end{figure}


\subsection{Implementierung}
Der \textit{VoxelFilterNode} liest die
zuvor per \textit{CropBox} zugeschnittene Punktwolke auf \texttt{points\_cropped}, setzt in
der PCL-Implementierung \texttt{pcl::VoxelGrid<pcl::PointXYZI>} die Blattgröße
\texttt{voxel\_size} in allen drei Achsen und führt anschließend die Unterabtastung aus.
Dadurch wird pro belegtem Voxel exakt ein Punkt in die ausgegebene, unterabgetastete
Punktwolke \texttt{points\_voxel} übernommen; die Rechenkomplexität sinkt damit auf
\(\mathcal{O}(n_\text{voxel})\), was
insbesondere die nachgelagerte k-d-Baum-Suche beim Clustering entlastet. Eine zu große
Voxelkante würde jedoch feine Strukturen (z.\,B. Fahrradspeichen oder dünne Leitpfosten)
verschmieren, während eine zu kleine Kante kaum Rechenzeit spart und die Cluster-Parameter
\(\varepsilon\) anheben würde (vgl. Abschnitt~\ref{chap:clustering}).

Die Startwahl \texttt{voxel\_size} = \SI{0.20}{\meter} orientiert sich sowohl an Literatur als
auch an den Tests aus Kapitel~\ref{chap:test_und_bewertung}. Lyu~u.\,a. \parencite{lyu2024voxel} untersuchen das
Voxel-Downsampling in urbanen Karten und zeigen, dass Blattgrößen im Bereich
\SI{0.15}{\meter}--\SI{0.25}{\meter} das Rauschen signifikant reduzieren, ohne Fußgänger und
Fahrräder zu verlieren; \SI{0.20}{\meter} erweist sich dort als stabiler Kompromiss. Die wichtigsten Startparameter
sind in Tabelle~\ref{tab:voxel_params} zusammengefasst.

% préambule : \usepackage{booktabs} (optionnel, pour un rendu plus pro)
\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \textbf{Parametername} & \textbf{Startwert} \\ \hline
    \texttt{input\_topic}  & \texttt{points\_cropped} \\ \hline
    \texttt{output\_topic} & \texttt{points\_voxel} \\ \hline
    \texttt{voxel\_size}   & \(\,0{,}20\,\mathrm{m}\,\) \\ \hline
  \end{tabular}
  \caption{Startparameter des VoxelGrid-Filters}
  \label{tab:voxel_params}
\end{table}

\subsection{Ergebnis}
Wie in Abbildung~\ref{fig:voxel_compare} dargestellt, führt die Anwendung des
VoxelGrid-Filters zu einer gleichmäßigeren und deutlich reduzierten Punktdichte,
ohne dass dabei wichtige Strukturen der Szene verloren gehen.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/voxel_compare.png}
  \caption{Vergleich nach \textit{CropBox} (links) und nach \textit{VoxelGrid-Filter} (rechts)}
  \label{fig:voxel_compare}
\end{figure}


\chapter{Bodensegmentierung}
\label{chap:bodensegmentierung}

Die Bodensegmentierung ist ein essenzieller Vorverarbeitungsschritt innerhalb der Verarbeitungskette zur Objekterkennung. 
Ihr Ziel besteht darin, Bodenpunkte zuverlässig von Hindernissen und anderen Objekten zu trennen, um die nachfolgenden Schritte – insbesondere die Cluster- und Objekterkennung – zu erleichtern. 
Im Folgenden werden zunächst die Anforderungen an die Bodensegmentierung beschrieben, anschließend verschiedene Methoden anhand definierter Bewertungskriterien verglichen und abschließend die in dieser Arbeit implementierte Methode vorgestellt.


\section{Anforderungen an der Bodensegmentierung}
\label{sec:anforderungen_bodenseg}

Die wichtigsten Anforderungen an ein Bodensegmentierungsverfahren lassen sich nach~\cite{gomes2023survey} in mehreren Kriterien zusammenfassen, 
die in Tabelle~\ref{tab:anforderungen_bodensegmentierung} dargestellt sind.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3} % augmente l'espacement vertical
\setlength{\tabcolsep}{8pt}       % ajuste l'espacement horizontal
\begin{tabular}{|p{4cm}|p{9.5cm}|}
\hline
\textbf{Kriterium} & \textbf{Beschreibung} \\ \hline

\textbf{Echtzeitfähigkeit} & 
Das Verfahren muss Punktwolken in Echtzeit verarbeiten können (z.\,B. <100\,ms pro Frame), 
um eine kontinuierliche Fahrzeugsteuerung zu ermöglichen. \\ \hline

\textbf{Rechenaufwand} & 
Geringe Rechen- und Speicheranforderungen sind notwendig, 
da die Algorithmen häufig auf eingebetteten Systemen mit begrenzten Ressourcen laufen. \\ \hline

\textbf{Segmentierungs\-robustheit} & 
Das Verfahren sollte robust gegenüber Über- und Untersegmentierung sein 
und Bodenpunkte korrekt von Hindernissen trennen. \\ \hline

\textbf{Leistung bei steigenden Hindernissen} & 
Die Methode sollte sanft ansteigende Strukturen (z.\,B. Rampen oder Bordsteine) 
korrekt als Teil des Bodens erkennen. \\ \hline

\textbf{Leistung bei unebenem Gelände} & 
Auch bei Neigungen oder unregelmäßigen Bodenoberflächen 
muss die Bodenschätzung stabil bleiben. \\ \hline

\textbf{Leistung bei spärlichen Daten} & 
Das Verfahren sollte bei geringer Punktdichte (z.\,B. großer Abstand zum Sensor) 
zuverlässige Ergebnisse liefern. \\ \hline
\end{tabular}

\vspace{5pt}
\caption{Hauptanforderungen an die Bodensegmentierung (nach~\cite{gomes2023survey})}
\label{tab:anforderungen_bodensegmentierung}
\end{table}

Diese Arbeit fokusiert sich auf vier Hauptkriterien:

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|p{4cm}|p{9cm}|}
\hline
\textbf{Nr.} & \textbf{Kriterium} & \textbf{Beschreibung} \\ \hline
1 & Echtzeitfähigkeit & Bewertet, ob die Methode unter den gegebenen Rechenbedingungen (10\,Hz Sensorrate) eine kontinuierliche Verarbeitung der Punktwolke in Echtzeit ermöglicht. \\ \hline
2 & Robustheit & Misst die Widerstandsfähigkeit gegenüber Rauschen, Ausreißern und unterschiedlichen Geländetypen (z.\,B. unebenes Terrain oder variable Punktdichten). \\ \hline
3 & Genauigkeit & Beschreibt die Fähigkeit der Methode, die Bodenpunkte präzise von Nicht-Bodenpunkten zu trennen und somit eine zuverlässige Segmentierung zu gewährleisten. \\ \hline
4 & Rechenaufwand & Bewertet den benötigten Ressourcenverbrauch (CPU/GPU-Zeit und Speicherbedarf) für die Durchführung der Bodensegmentierung. \\ \hline
\end{tabular}
\caption{Bewertungskriterien zur Analyse der Bodensegmentierungsmethoden.}
\label{tab:bewertungskriterien}
\end{table}

Die Auswahl dieser vier Bewertungskriterien basiert auf den Anforderungen, 
die bei der Verarbeitung von LiDAR-Daten in urbanen Umgebungen besonders relevant sind.  
In städtischen Szenarien treten komplexe Strukturen, wechselnde Oberflächenmaterialien 
und zahlreiche bewegte Objekte auf, was eine hohe Robustheit und Genauigkeit bei der 
Bodensegmentierung erfordert.  
Gleichzeitig muss die Verarbeitung der Sensordaten in Echtzeit erfolgen, 
um eine kontinuierliche Umfeldwahrnehmung und gegebenenfalls eine sichere Fahrzeugführung 
zu gewährleisten.  
Da in praktischen Anwendungen häufig nur begrenzte Rechenressourcen zur Verfügung stehen,
ist auch der Rechenaufwand ein entscheidender Faktor für die Auswahl geeigneter Verfahren.
Diese vier Kriterien bilden somit die Grundlage für eine objektive Bewertung
und den methodischen Vergleich der in dieser Arbeit betrachteten Ansätze.

Die sechs Anforderungen aus Tabelle~\ref{tab:anforderungen_bodensegmentierung} werden in Tabelle~\ref{tab:bewertungskriterien}
zu vier Bewertungskriterien zusammengeführt, indem inhaltlich verwandte Punkte gebündelt werden:
\begin{itemize}
    \item \textbf{Robustheit} fasst die Leistungsbewertung bei steigenden Hindernissen, unebenem Gelände und spärlichen Daten zusammen,
    weil alle drei Aspekte die Widerstandsfähigkeit gegenüber unterschiedlichen Gelände- und Sensordichtevariationen abprüfen.
    \item \textbf{Genauigkeit} nimmt den Aspekt der korrekten Trennung von Boden- und Nicht-Bodenpunkten aus der Segmentierungsrobustheit auf,
    um die resultierende Segmentierungsqualität klar zu bewerten.
    \item \textbf{Echtzeitfähigkeit} und \textbf{Rechenaufwand} bleiben als eigenständige Kriterien erhalten,
    da sie die Laufzeit- und Ressourcenanforderungen unabhängig von den Szenarioeigenschaften adressieren.
\end{itemize}

\section{Methoden der Bodensegmentierung}
\label{chap:bodenmethoden}

Nach~\cite{gomes2023survey} lassen sich die gängigen Verfahren zur Bodensegmentierung
in fünf Hauptkategorien einteilen (siehe Abbildung~\ref{fig:boden_taxonomie}):

\begin{itemize}
    \item 2.5D-Gitterbasierte Verfahren
    \item Bodenmodellierung (Ground Modelling)
    \item Methoden auf Basis benachbarter Punkte und lokaler Merkmale
    \item Verfahren höherer Ordnung (Higher-Order Inference)
    \item Lernbasierte Verfahren (Deep Learning)
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Bilder/boden_taxonomie.png}
  \caption{Klassifizierung und Systematisierung bestehender Bodensegmentierungsmethoden}
  \label{fig:boden_taxonomie}
\end{figure}


Diese Klassifikation deckt sowohl klassische geometrische Ansätze als auch moderne, neuronale Verfahren ab. 
Im Folgenden werden die wichtigsten Prinzipien und repräsentativen Algorithmen jeder Kategorie erläutert.

\subsection{2.5D-Gitterbasierte Verfahren}
% ============================

Ein verbreiteter Ansatz zur Bodensegmentierung besteht darin, die dreidimensionale Punktwolke in eine zweidimensionale Rasterdarstellung zu überführen.  
Dabei werden die Punkte nach ihren Koordinaten in diskrete Zellen eingeteilt, sodass jede Zelle statistische Höheninformationen über die in ihr enthaltenen Punkte speichert.  
Dieser sogenannte 2.5D-Ansatz reduziert die Komplexität der Verarbeitung erheblich, da die Analyse nicht mehr im vollen 3D-Raum, sondern auf einer strukturierten Gitterebene erfolgt.

Douillard et al.~\cite{douillard2011segmentation} präsentieren ein solches Verfahren auf Basis von \textit{Elevation Maps}, bei dem jede Gitterzelle durch den Mittelwert der Höhenwerte ihrer Punkte beschrieben wird.  
Zellen mit geringer Höhenvarianz werden als Boden klassifiziert, während größere Abweichungen auf Objekte oder Hindernisse hinweisen.  
Durch diese Reduktion auf lokale Höhenstatistiken kann die Methode Bodenflächen effizient und robust in urbanen Umgebungen identifizieren, ohne den gesamten Punktwolkenraum verarbeiten zu müssen.


% ============================
\subsection{Bodenmodellierung (Ground Modelling)}
% ============================

Diese Methoden approximieren die Bodenfläche durch mathematische Modelle, typischerweise in Form von Ebenen oder Linien, um die Trennung zwischen Boden- und Nichtbodenpunkten zu ermöglichen.

\textbf{Plane Fitting:}  
In \cite{hu2013robust} wird die Identifikation von Bodenpunkten mithilfe der \textit{Random Sample Consensus (RANSAC)}-Methode beschrieben, 
bei der eine Ebene an die niedrigsten Punkte angepasst und Punkte mit geringem orthogonalen Abstand als \textit{Inlier} klassifiziert werden.  
Ein ähnlicher Ansatz teilt die Punktwolke in konzentrische Zonen auf und passt für jede Zone lokal angepasste Ebenen an, 
wobei die \textit{Principal Component Analysis (PCA)} eingesetzt wird, um die Hauptrichtung der Punktverteilung zu bestimmen und daraus die bestmögliche Ebenenorientierung zu berechnen \cite{lim2020fast}.  
Dieser Ansatz erreicht eine hohe Präzision (F1-Score $\approx 0.93$) bei gleichzeitigem Echtzeitverhalten.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/plane_fitting.png}
  \caption{Visuelle Darstellung einer orthogonalen Distanzklassifizierung (~\cite{gomes2023survey})}
  \label{fig:plane_fitting}
\end{figure}

\textbf{Linienextraktion:}  
Ein auf lokalen Linienanpassungen basierendes Verfahren modelliert die entlang eines Laserstrahls erfassten Punkte als nahezu linearen Verlauf \cite{himmelsbach2010fast}.  
Dieses Verfahren ist recheneffizient und eignet sich für Echtzeitverarbeitung, weist jedoch Schwächen in stark unebenem oder komplexem Gelände auf.

\textbf{Gaussian Process Regression (GPR):}  
Die \textit{Gaussian Process Regression (GPR)} wurde zur Modellierung der Bodenhöhe eingeführt, 
wobei der Höhenverlauf als kontinuierliche Funktion mit zugehöriger Unsicherheitsabschätzung beschrieben wird \cite{douillard2011segmentation}.  
Durch die Verwendung nichtstationärer Kovarianzfunktionen kann sich das Modell lokal an unterschiedliche Geländestrukturen anpassen und dadurch eine hohe Genauigkeit auch in unregelmäßigem Terrain erreichen \cite{chen2014real}.

% ============================
\subsection{Benachbarte Punkte und lokale Merkmale}
% ============================

Diese Algorithmen analysieren geometrische Beziehungen zwischen benachbarten Punkten in der Punktwolke.  
Dabei wird häufig die vertikale Kanalstruktur moderner LiDAR-Sensoren (z.\,B. Velodyne VLP-16 oder Ouster OS1-64) ausgenutzt, um lokale Abhängigkeiten entlang der Scanlinien zu erkennen.

\textbf{Kanalbasierte Verfahren:}  
In \cite{chu2019ground} wird die Bodenextraktion entlang vertikaler Scans beschrieben, 
bei der lokale Höhenunterschiede und Gradienten ausgewertet werden.  
Punkte zwischen einem Startbodenpunkt und einem definierten Schwellenwert werden als Boden klassifiziert.  
Das Verfahren ist recheneffizient, reagiert jedoch empfindlich auf eine ungenaue Parametrierung, etwa bei der Wahl des Höhen- oder Gradientschwellenwerts.

\textbf{Region-Growing und Clustering:}  
In \cite{moosmann2009segmentation} wird ein graphenbasiertes Region-Growing-Verfahren vorgestellt, 
bei dem benachbarte Punkte iterativ zu Regionen zusammengefügt werden, 
sofern lokale geometrische Kriterien (z.\,B. Konvexität) erfüllt sind.  
Ein alternativer Ansatz kombiniert voxelbasiertes Clustering mit statistischer Analyse, 
um Bodencluster zuverlässig zu isolieren \cite{douillard2011segmentation}.  
Solche Methoden liefern stabile Ergebnisse auch in komplexen Szenen, 
sind jedoch rechenintensiver und daher weniger für Echtzeitanwendungen geeignet.

\textbf{Range-Image-Methoden:}  
In \cite{bogoslavskyi2016fast} wird die Punktwolke in ein zweidimensionales Entfernungsbild (\textit{Range Image}) projiziert, 
bei dem jeder Pixel den Abstand eines Messpunkts zum Sensor repräsentiert.  
Diese Repräsentation ermöglicht eine effiziente Definition von Nachbarschaften und vereinfacht die anschließende Segmentierung erheblich.  
Mit diesem Ansatz kann die Bodenextraktion in wenigen Millisekunden pro Frame durchgeführt werden, was eine Echtzeitverarbeitung erlaubt.

% ============================
\subsection{Verfahren höherer Ordnung}
% ============================

Ansätze dieser Kategorie verwenden probabilistische Graphmodelle wie 
\textit{Markov Random Fields (MRF)} oder \textit{Conditional Random Fields (CRF)}, 
um Abhängigkeiten zwischen benachbarten Punkten explizit zu modellieren.  
Durch die Berücksichtigung solcher räumlicher Korrelationen können Fehlklassifikationen, 
insbesondere in spärlichen oder verrauschten Punktwolken, deutlich reduziert werden.

In \cite{guo2011ground} wird ein MRF-Modell mit dem \textit{Belief Propagation (BP)}-Verfahren kombiniert, 
das die Wahrscheinlichkeiten einzelner Punktzuordnungen iterativ aktualisiert, 
um eine konsistente Bodenfläche auch in unebenem Gelände zu rekonstruieren.  
Ein weiterentwickelter Ansatz integriert zeitliche Abhängigkeiten in ein CRF-Modell, 
wodurch die Konsistenz zwischen aufeinanderfolgenden Frames verbessert 
und die Stabilität der Segmentierung bei Bewegungen erhöht wird \cite{rummelhard2015temporal}.

% ============================
\subsection{Lernbasierte Verfahren}
% ============================

In den letzten Jahren haben sich tief neuronale Netze als besonders leistungsfähige Ansätze für die Punktwolkensegmentierung etabliert.  
Je nach Architektur werden die Sensordaten entweder direkt in Punktform verarbeitet oder zuvor in strukturierte Darstellungen überführt, um eine effiziente Merkmalsextraktion zu ermöglichen.

\textbf{PointNet- und Voxel-basierte Modelle:}  
Das in \cite{qi2017pointnet} vorgestellte PointNet-Framework ermöglicht die direkte Verarbeitung unstrukturierter Punktwolken, indem es für jeden Punkt Merkmale extrahiert und diese global aggregiert.  
Zur Erfassung lokaler geometrischer Abhängigkeiten wurde das Konzept in PointNet++ erweitert.  
Alternativ unterteilen voxelbasierte Verfahren wie VoxelNet \cite{zhou2018voxelnet} oder PointPillars \cite{lang2019pointpillars} die Punktwolke in diskrete 3D-Zellen (Voxel) bzw. Säulen und verwenden dreidimensionale Faltungsnetzwerke (3D-CNNs) zur Merkmalsanalyse.  

\textbf{Bildbasierte Ansätze:}  
In \cite{wu2018squeezeseg} und \cite{milioto2019rangenet++} werden Punktwolken in zweidimensionale Entfernungsbilder (\textit{Range Images}) projiziert, sodass konventionelle 2D-Faltungsnetzwerke auf LiDAR-Daten angewendet werden können.  
Diese Repräsentation ermöglicht eine Echtzeitverarbeitung auf GPUs bei gleichzeitig hoher Segmentierungsgenauigkeit.  

\textbf{Spezialisierte Netze für Bodensegmentierung:}  
Ein speziell für die Bodenerkennung entwickeltes neuronales Modell ist GndNet \cite{paigwar2020gndnet}.  
Das Netz basiert auf einem zweidimensionalen Gittermodell, in dem für jede Zelle die Bodenhöhe vorhergesagt wird.  
Dieses Verfahren erreicht eine mittlere Intersection-over-Union (IoU) von 83,6\,\% bei einer Laufzeit von nur 17,9\,ms pro Frame und ist somit für Echtzeitanwendungen geeignet.

% ============================
\subsection{Zusammenfassung}
% ============================

Tabelle~\ref{tab:vergleich_bodenmethoden} fasst die wesentlichen Eigenschaften der vorgestellten Bodensegmentierungsmethoden zusammen.  
Sie verdeutlicht die jeweiligen Stärken und Grenzen der Ansätze sowie ihre typischen Einsatzgebiete in der mobilen Robotik und Umfelderkennung.

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|p{3.5cm}|p{4cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Methodenkategorie} & \textbf{Vorteile} & \textbf{Nachteile} & \textbf{Typisches Einsatzgebiet} \\ \hline
2.5D-Gitterbasierte Verfahren & Geringe Rechenlast, robust auf ebenem Gelände & Begrenzte Genauigkeit bei Überhängen oder Brücken & Stadt- und Straßenszenarien \\ \hline
Bodenmodellierung & Hohe Präzision, einfache mathematische Umsetzung & Geringe Robustheit bei komplexen Oberflächenformen & Flaches bis leicht geneigtes Terrain \\ \hline
Lokale Merkmalsanalyse & Unempfindlich gegenüber Dichteänderungen & Starke Abhängigkeit von Parameterwahl und Sensorgeometrie & Dynamische oder unstrukturierte Umgebungen \\ \hline
Probabilistische Graphmodelle (MRF/CRF) & Hohe Konsistenz durch Nachbarschaftsbeziehungen & Hohe Rechenkomplexität, geringe Echtzeitfähigkeit & Forschungs- und Entwicklungsumgebungen \\ \hline
Lernbasierte Verfahren & Sehr hohe Genauigkeit, anpassungsfähig durch Training & Großer Trainings- und Hardwareaufwand & Autonomes Fahren, Echtzeit-Perzeption \\ \hline
\end{tabular}
\caption{Vergleich der Bodensegmentierungsmethoden in Bezug auf Rechenaufwand, Robustheit und Einsatzgebiet (nach~\cite{gomes2023survey}).}
\label{tab:vergleich_bodenmethoden}
\end{table}

\begin{table}[h!]
  \centering
  \renewcommand{\arraystretch}{1.2}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
      \hline
      \textbf{Methode} & \textbf{Echtzeitfähigkeit} & \textbf{Robustheit} & \textbf{Genauigkeit} & \textbf{Rechenaufwand} \\
      \hline
      2.5D-Gitterbasierte Verfahren & Hoch (Zellstatistiken pro Raster) & Mittel (stabil bei moderaten Höhenänderungen) & Mittel (verlustbehaftete 2.5D-Projektion) & Gering \\ \hline
      RANSAC-Ebenenanpassung (Plane Fitting) & Mittel (Iterationsbudget begrenzt Laufzeit) & Hoch (ausreißerresistent) & Hoch (präzise Ebenenmodellierung) & Mittel bis hoch \\ \hline
      Linienextraktion entlang Scanlinien & Hoch (lineare Fits pro Strahl) & Niedrig bis mittel (empfindlich auf unebenes Gelände) & Mittel & Sehr gering \\ \hline
      Gaussian Process Regression (GPR) & Niedrig (aufwändige Kernelberechnung) & Hoch (passt sich lokalem Terrain an) & Hoch & Hoch \\ \hline
      Kanalbasierte Verfahren (gradientenbasiert) & Hoch (schlanke Schwellenwertlogik) & Mittel (parametergängig, sensorgeometrieabhängig) & Mittel & Gering \\ \hline
      Region-Growing / graphenbasiertes Clustering & Niedrig bis mittel (iterative Nachbarschaftsprüfung) & Mittel bis hoch (reduziert Fehlklassifikationen) & Hoch & Mittel bis hoch \\ \hline
      Range-Image-Methoden & Hoch (2D-Nachbarschaften in Millisekunden) & Mittel (robust gegen Rauschen, sensibel bei Überhängen) & Mittel bis hoch & Gering bis mittel \\ \hline
      Probabilistische Graphmodelle (MRF/CRF) & Niedrig (iterative Inferenz) & Hoch (konsistente Nachbarschaftsmodellierung) & Hoch & Hoch \\ \hline
      Point-/Voxel-basierte Netze (PointNet, PointPillars) & Mittel (GPU-Echtzeit bei optimierter Implementierung) & Hoch (lernbasierte Generalisierung) & Sehr hoch & Hoch \\ \hline
      Bildbasierte CNNs auf Range Images (z.B. RangeNet++) & Hoch (2D-CNN-Inferenz auf GPU) & Mittel bis hoch (robust auf strukturierten Projektionen) & Hoch & Mittel bis hoch \\ \hline
      Spezialisierte Netze (GndNet) & Hoch (Inference $\approx$ 18\,ms/Frame) & Mittel bis hoch (geländeadaptiv trainierbar) & Hoch (IoU $\approx$ 84\,\%) & Mittel \\ \hline
    \end{tabular}%
  }
  \caption{Vergleich der in Abschnitt~\ref{chap:bodenmethoden} beschriebenen Bodensegmentierungsmethoden nach Echtzeitfähigkeit, Robustheit, Genauigkeit und Rechenaufwand (nach~\cite{gomes2023survey}).}
  \label{tab:boden-methodenvergleich-ransac}
\end{table}


Das RANSAC-Verfahren zeichnet sich durch eine hohe Robustheit gegenüber Ausreißern aus und liefert eine präzise Anpassung planarer Bodenflächen.  
Im Vergleich zu rasterbasierten oder morphologischen Methoden ermöglicht es eine genauere Modellierung, erfordert jedoch einen höheren Rechenaufwand und eine sorgfältige Parametrierung, um eine stabile Echtzeitverarbeitung zu gewährleisten.  
Aufgrund seiner Balance zwischen Präzision und algorithmischer Einfachheit wurde RANSAC als Grundlage für die in dieser Arbeit implementierte Bodensegmentierung ausgewählt.

\section{Implementierung}

\label{sec:implementierung_ransac}

Die Bodensegmentierung wurde als eigenständiger ROS~2-Knoten in \texttt{C++} mit der \textit{Point Cloud Library (PCL)} implementiert. 
Der Knoten \texttt{ransac\_ground\_node} abonniert gefilterte LiDAR-Punktwolken, schätzt eine Bodenebene mittels \textit{SAC-RANSAC} und veröffentlicht eine Hindernis-Punktwolke, aus der die Bodenpunkte entfernt wurden. 
Die Architektur ist strikt streaming-orientiert (Callback-basiert) und verzichtet auf Blockierungen, wodurch eine niedrige Latenz erzielt wird.

Der Knoten deklariert die Ein- und Ausgabetopics als Parameter und nutzt \texttt{SensorDataQoS}:
\begin{itemize}
  \item \textbf{Eingabe (\texttt{input\_topic})}: \texttt{/points\_voxel} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält bereits per VoxelGrid ausgedünnte Punkte (\texttt{pcl::PointXYZI}).
  \item \textbf{Ausgabe (\texttt{output\_topic})}: \texttt{/obstacle\_points} \ \emph{(Typ: \texttt{sensor\_msgs/PointCloud2})}, enthält ausschließlich Nicht-Boden-Punkte.
\end{itemize}
Leere Eingaben werden verworfen, um unnötige Rechenarbeit zu vermeiden.

Die wesentlichen Laufzeitparameter werden als ROS-Parameter deklariert und können über Launch-Dateien oder \texttt{ros2 param} angepasst werden (Default-Werte aus dem Code):
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|l|}
\hline
\textbf{Parameter} & \textbf{Default} & \textbf{Bedeutung} \\ \hline
\texttt{input\_topic} & \texttt{/points\_voxel} & Eingangs-Punktwolke (vorsegmentiert per VoxelGrid) \\ \hline
\texttt{output\_topic} & \texttt{/obstacle\_points} & Ausgabe ohne Bodenpunkte \\ \hline
\texttt{distance\_threshold} & $0{,}15\,\mathrm{m}$ & maximaler Punkt-zu-Ebene-Abstand für Inlier \\ \hline
\texttt{max\_iterations} & $1000$ & maximale RANSAC-Iterationen \\ \hline
\end{tabular}
\caption{RANSAC-Parametrisierung des \texttt{ransac\_ground\_node}.}
\label{tab:ransac_params}
\end{table}

Zur Ebenenschätzung wird \texttt{pcl::SACSegmentation} mit \texttt{SACMODEL\_PLANE} und \texttt{SAC\_RANSAC} verwendet, inkl.\ Koeffizientenoptimierung:
\begin{enumerate}
  \item \textbf{Konvertierung}: \texttt{sensor\_msgs/PointCloud2} $\rightarrow$ \texttt{pcl::PointCloud<pcl::PointXYZI>}.
  \item \textbf{RANSAC-Konfiguration}: \texttt{setOptimizeCoefficients(true)}, \texttt{setModelType(PLANE)}, \texttt{setMethodType(RANSAC)}, \texttt{setDistanceThreshold}, \texttt{setMaxIterations}.
  \item \textbf{Segmentierung}: \texttt{seg.segment(inliers, coefficients)} liefert Inlier-Indizes der Bodenpunkte und Ebenenparameter $\mathbf{n}=(a,b,c)$, $d$ der Ebene
  \[
    a x + b y + c z + d = 0.
  \]
  \item \textbf{Extraktion der Hindernisse}: \texttt{pcl::ExtractIndices} mit \texttt{setNegative(true)} filtert alle Punkte \emph{außerhalb} der Bodenenebene (\,$>$ \texttt{distance\_threshold}\,).
  \item \textbf{Publikation}: Rückkonvertierung nach \texttt{PointCloud2} und Veröffentlichung auf \texttt{/obstacle\_points} (mit ursprünglichem Header/Frame).
\end{enumerate}
Ein Punkt $\mathbf{p}=(x,y,z)^\top$ zählt als Inlier, wenn sein orthogonaler Abstand zur Ebene
\[
  \mathrm{dist}(\mathbf{p}, \Pi) \;=\; \frac{|a x + b y + c z + d|}{\sqrt{a^2+b^2+c^2}}
\]
kleiner gleich \texttt{distance\_threshold} ist.

\section{Ergebnis}
Als Ergebnis ergibt sich eine Punktwolke, die frei von Bodenpunkten ist (siehe Abbildung~\ref{fig:ransac_compare}). 
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/ransac_compare.png}
  \caption{Vergleich der ursprünglichen Punktwolke (links) und der nach dem RANSAC-Algorithmus gefilterten Punktwolke (rechts)}
  \label{fig:ransac_compare}
\end{figure}

\chapter{Cluster-Extraktion und Ermittlung von Bounding-Boxes}
\label{chap:clustering}
\label{chap:cluster_extraction}

Nach der Entfernung der Bodenpunkte (vgl. Kapitel~\ref{chap:bodensegmentierung}) werden die verbleibenden Punktwolken in zusammenhängende Punktmengen (Cluster) gruppiert und mit Begrenzungsboxen beschrieben. Ziel ist eine in Echtzeit lauffähige, robuste Objektbildung, die konsistente Boxen (Lage, Abmessungen) für die nachgelagerte Verfolgung bereitstellt.

\section{Cluster-Extraktion}
\label{sec:prinzip_cluster_boxes}
\label{sec:cluster_extraction}
Zwei Punkte \(p_i=(x_i,y_i,z_i)^\top\) und \(p_j\) gehören demselben Cluster an, wenn sie in einem geeigneten Nachbarschaftsbegriff als verbunden gelten. Daraus lässt sich ein ungerichteter Graph \(G=(V,E)\) konstruieren, dessen Knoten \(V\) die Punkte und dessen Kanten \(E\) die Paarverbindungen darstellen. Die Zusammenhangskomponenten dieses Graphen entsprechen den Clustern; Traversierungen (BFS/DFS) über einen k-d-Baum liefern in \(O(n \log n)\) die Komponenten \parencite{rusu2011pcl,Douillard2011}. Im gängigen euklidischen Ansatz bildet die Distanz
\[
 d(p_i,p_j) = \sqrt{(x_i-x_j)^2+(y_i-y_j)^2+(z_i-z_j)^2}
\]
die Grundlage; eine feste Verknüpfungsschwelle \(\varepsilon\) definiert die Kanten von \(G\). LiDAR-Punktwolken besitzen aufgrund ihres winkelbasierten Abtastgitters eine von der Entfernung abhängige, anisotrope Punktdichte \parencite{himmelsbach2010fast}. Deshalb beeinflussen Vorverarbeitung (Voxelgröße) und die Wahl von \(\varepsilon\) die Fragmentierung/Verschmelzung besonders stark: Ein zu kleiner Schwellenwert lässt weit entfernte Objekte zerfallen, ein zu großer führt zum Verschmelzen benachbarter Objekte im Nahbereich. Abbildung~\ref{fig:cluster_principle} skizziert das Vorgehen und verdeutlicht, dass die Clusterung die eigentliche Objektbildung für die nachgelagerte Verfolgung bereitstellt.

Für jede identifizierte Komponente werden im Anschluss Lage (Zentrum) und Ausdehnung über Begrenzungsboxen bestimmt. Achsenparallele Boxen (AABB) greifen auf Min-/Max-Koordinaten zurück, orientierte Boxen (OBB) nutzen Trägheitstensor oder Hauptachsentransformation (PCA), um die Orientierung der Punktverteilung widerzuspiegeln \parencite{gottschalk1996obb}. Beide Varianten liefern damit ein abstrahiertes, metrisches Objektmodell, das als \emph{Detektion} publiziert wird und die Initialisierung der Tracking-Stufe ermöglicht.

\begin{figure}[H]
  \centering
  % Platzhalter — später durch Illustration ersetzen
  \fbox{\parbox{0.9\linewidth}{\centering
    Platzhalter: Schemadarstellung — Eingangspunktwolke → farbige Cluster → Boxen.
  }}
  \caption{Prinzip: euklidische Clusterung und nachgelagerte Box-Ermittlung.}
  \label{fig:cluster_principle}
\end{figure}

In dieser Arbeit wird die euklidische Clusterung der \emph{Point Cloud Library (PCL)} eingesetzt (\texttt{pcl::EuclideanClusterExtraction}) in Verbindung mit einem k-d-Baum (\texttt{pcl::search::KdTree}) \parencite{pcl_docs_2025}. Der Algorithmus folgt dem in der Literatur etablierten Ablauf: Aufbau eines Suchbaums, breadth-first/ depth-first Traversierung und komponentenweise Markierung der Punkte \parencite{Douillard2011,rusu2011pcl}. Gegenüber dichtebasierten Verfahren wie DBSCAN sind die Parameterzahl und die Rechenzeit deterministischer, was die Integration in eine Echtzeit-Pipeline erleichtert \parencite{Ester1996DBSCAN}. Zusätzlich ist das Verfahren trainingsfrei und benötigt nur wenige, gut interpretierbare Parameter:

\begin{itemize}
  \item \textbf{\texttt{cluster\_tolerance}} \(\varepsilon\) [m]: maximaler euklidischer Punktabstand innerhalb eines Clusters (räumliche Verknüpfung).
  \item \textbf{\texttt{min\_cluster\_size}} / \textbf{\texttt{max\_cluster\_size}}: untere/obere Schranke der Punktanzahl pro Cluster (Rauschen verwerfen, Ausreißer begrenzen).
  \item \textbf{\texttt{max\_clusters}}: harte Obergrenze pro Frame (Determinismus, Zeitbudget).
\end{itemize}

Richtwerte im Zusammenspiel mit einem Voxel-Filter (Voxelkantenlänge \(\approx\)\SI{0.15}{\meter}–\SI{0.25}{\meter}):
\(\varepsilon \in [\SI{0.3}{\meter},\SI{0.7}{\meter}]\), \texttt{min\_cluster\_size} \(\approx 30\)–\(60\), \texttt{max\_clusters} \(\approx 200\). Solche Werte liegen im Bereich typischer Automotive-Studien, in denen \(\varepsilon\) zwischen \SI{0.4}{\meter} und \SI{0.8}{\meter} gewählt wird, um Fahrzeuge und Fußgänger bei mittlerer Voxelung zu trennen \parencite{himmelsbach2010fast,Douillard2011}. Kleinere \(\varepsilon\) vermeiden Verschmelzen benachbarter Objekte, größere \(\varepsilon\) reduzieren Fragmentierung in der Ferne. Alternativen wie dichtebasierte Verfahren (DBSCAN/HDBSCAN) sind möglich, werden hier zugunsten der Echtzeitfähigkeit und einfachen Parametrik jedoch nicht eingesetzt \parencite{Ester1996DBSCAN}.

\paragraph{Ablauf (Algorithmus)}
\begin{enumerate}
  \item \textbf{Vorbereitung}: optionales Downsampling per Voxel (gleichmäßigere Dichte, O(\(n\))).
  \item \textbf{k-d-Baum}: Aufbau für die reduzierte Punktwolke (O(\(n\log n\))).
  \item \textbf{Region Growing}: iteratives Durchlaufen unbesuchter Punkte; Nachbarsuche innerhalb \(\varepsilon\) über k-d-Baum, Zusammenfassen zu Komponenten (typisch \(\in O(n\log n)\)).
  \item \textbf{Selektion}: Verwerfen zu kleiner/großer Komponenten per \texttt{min/max\_cluster\_size}.
\end{enumerate}

\paragraph{Praktische Aspekte}
- \emph{Komplexität}: der Flaschenhals ist die Nachbarsuche; Voxel-Dowsampling reduziert \(n\) und beschleunigt die Suche signifikant \parencite{rusu2011pcl}.
- \emph{Parameterkopplung}: sinnvolle Werte von \(\varepsilon\) korrelieren mit der Voxelgröße. Faustregel: \(\varepsilon\approx 2\dots 3\)\,\(\times\)\,\texttt{voxel\_size}; bei anisotroper Punktdichte (winkelbasiertes Abtastgitter) verstärkt sich dieser Effekt \parencite{himmelsbach2010fast}.
- \emph{Reichweiten-Adaption (optional)}: \(\varepsilon(r)=\alpha r+\beta\) kann Fernbereichs-Fragmentierung reduzieren, wie in Reichweiten-bewussten Segmentierungen gezeigt \parencite{bogoslavskyi2016fast}; in dieser Arbeit verwenden wir für Determinismus und Einfachheit eine konstante \(\varepsilon\).

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|p{8cm}|}
    \hline
    \textbf{Parameter} & \textbf{Beispiel} & \textbf{Bedeutung} \\ \hline
    \texttt{cluster\_tolerance} & \(0{,}5\,\mathrm{m}\) & Verknüpfungsschwelle in der euklidischen Nachbarschaft \\ \hline
    \texttt{min\_cluster\_size} & 40 & Rauschunterdrückung, minimale Punktzahl \\ \hline
    \texttt{max\_cluster\_size} & 8000 & Obergrenze zur Begrenzung großer Flächen/Hecken \\ \hline
    \texttt{max\_clusters} & 200 & Harte Obergrenze pro Frame (Zeitbudget/Determinismus) \\ \hline
  \end{tabular}
  \caption{Parameter der euklidischen Clusterung (Richtwerte in den Experimenten).}
  \label{tab:cluster_params_new}
\end{table}

\begin{figure}[H]
  \centering
  % Platzhalter — später durch Plot/Heatmap ersetzen
  \fbox{\parbox{0.9\linewidth}{\centering
    Platzhalter: Einfluss von \(\varepsilon\) und Voxelgröße auf Clusteranzahl, BEV-IoU und Laufzeit.
  }}
  \caption{Parameter-Einfluss (Beispiel): \(\varepsilon\) (\texttt{cluster\_tolerance}) und Voxelgröße im Vergleich.}
  \label{fig:epsilon_sweep}
\end{figure}

\subsection{Ermittlung von Bounding-Boxes}
\label{sec:boxes}
Für jeden Cluster werden Zentrum und Ausdehnung über eine Begrenzungsbox bestimmt. Unterschieden werden achsenparallele Boxen (AABB) und orientierte Boxen (OBB). Beide Varianten liefern ein Zentrum \(\mathbf{c}\) und die Kantenlängen \((L, W, H)\); OBB enthält zusätzlich eine Orientierung \(\mathbf{q}\) (z.\,B. als Quaternion). Abbildung~\ref{fig:aabb_obb_compare} zeigt den praktischen Unterschied.

\begin{figure}[H]
  \centering
  % Platzhalter — später durch Vergleichsgrafik ersetzen
  \fbox{\parbox{0.9\linewidth}{\centering
    Platzhalter: Vergleich AABB (achsenparallel) vs. OBB (orientiert) am selben Cluster (Draufsicht/3D).
  }}
  \caption{Vergleich AABB vs. OBB an einem Beispielcluster.}
  \label{fig:aabb_obb_compare}
\end{figure}

Die Boxermittlung folgt auf die Punktzusammenfassung und nutzt ausschließlich Clusterpunkte. Für robuste Höhen \(H\) empfiehlt sich die Verwendung von Perzentilen (z.\,B. \(H=\mathrm{P}_{95}(z)-\mathrm{P}_{05}(z)\)), um vereinzelte Ausreißer zu dämpfen.

\subsubsection{AABB}
\label{sec:aabb}
Die AABB wird durch die minimalen und maximalen Koordinaten je Achse bestimmt:
\(x_{\min},x_{\max},y_{\min},y_{\max},z_{\min},z_{\max}\). Zentrum und Kantenlängen ergeben sich zu
\[\mathbf{c}=\tfrac{1}{2}\bigl((x_{\min},y_{\min},z_{\min})^\top+(x_{\max},y_{\max},z_{\max})^\top\bigr),\quad
 L=x_{\max}-x_{\min},\; W=y_{\max}-y_{\min},\; H=z_{\max}-z_{\min}.
\]
Vorteile: sehr geringe Rechenkosten, deterministisch und robust gegenüber Ausdünnung/Teilsicht. Nachteil: bei gedrehten Objekten überschätzt die AABB die Grundfläche in der Draufsicht (BEV), da die Orientierung nicht erfasst wird.

\subsubsection{OBB}
\label{sec:obb}
Die OBB richtet die Box entlang der Hauptachsen des Punktverteilungstensors aus. Praktisch wird dies in PCL über die Trägheits-/PCA-Schätzung realisiert (z.\,B. \texttt{pcl::MomentOfInertiaEstimation}) \parencite{pcl_docs_2025}. Vorteile: kompaktere Umhüllung bei deutlich orientierten, elongierten Objekten (Fahrzeuge, Leitplanken); die abgeleitete Yaw kann nachgelagert genutzt werden. Nachteile: höhere Rechenkosten und potenzielle Instabilitäten bei kleinen, teilverdeckten oder nahezu isotropen Clustern (Rauscheinfluss, Sprünge der Hauptachsen). Eine alternative, zweidimensionale Näherung ist die Bestimmung eines Minimalrechtecks in der Draufsicht (BEV), welches besonders stabile Yaw-Werte für straßennahe Objekte liefert; diese Option ist mit geringem Mehraufwand realisierbar und lässt sich mit AABB kombinieren (AABB für \(L,W,H\), Yaw aus BEV). Abbildung~\ref{fig:bev_minarea} reserviert Platz für eine entsprechende Darstellung.

\begin{figure}[H]
  \centering
  % Platzhalter — später durch BEV-MinArea-Beispiel ersetzen
  \fbox{\parbox{0.9\linewidth}{\centering
    Platzhalter: BEV-Minimalrechteck (Yaw) vs. AABB-Grundfläche im gleichen Ausschnitt (Straßenszene).
  }}
  \caption{BEV-Minimalrechteck zur robusten Yaw-Schätzung (Vergleich zur AABB-Grundfläche).}
  \label{fig:bev_minarea}
\end{figure}

\paragraph*{Vergleich AABB vs. OBB und Wahl in dieser Arbeit}
\begin{itemize}
  \item \textbf{AABB}: \emph{+} sehr schnell, stabil, deterministisch; \emph{−} keine Orientierung, tendenziell größere Grundfläche bei gedrehten Objekten.
  \item \textbf{OBB}: \emph{+} Orientierung/kompaktere Hülle bei elongierten Objekten; \emph{−} aufwändiger, empfindlicher gegenüber Ausdünnung/Teilsicht, potenziell sprunghaft.
\end{itemize}
In dieser Arbeit wird \textbf{AABB} verwendet. Hauptgründe sind die hohe Stabilität der Boxen unter realen Sensorbedingungen (variable Punktdichte, Okklusionen) und die geringen Rechenkosten, die die Echtzeitfähigkeit in der Gesamtkette begünstigen. Die Orientierung kann optional aus nachfolgenden Schritten (z.\,B. in BEV) geschätzt werden, ohne die Boxbildung zu destabilisieren. Eine OBB-Erweiterung ist als künftige Verbesserung vorgesehen, insbesondere wenn robuste Orientierungsmerkmale (z.\,B. BEV-MinArea) zuverlässig verfügbar sind.

\section{Implementierung}
\label{sec:implementierung_cluster_boxes}

Die Umsetzung erfolgt auf Basis von PCL und ROS~2 als durchgängige Verarbeitungskette. Auf dem Topic \texttt{/obstacle\_points} wird die \emph{EuclideanClusterExtraction} mit einer KdTree-Struktur angewendet, um Punktwolken in einzelne Objekte zu segmentieren. Die resultierenden Cluster werden anschließend sowohl als Marker für RViz visualisiert als auch in Form eines \texttt{vision\_msgs/Detection3DArray} veröffentlicht. Die Verarbeitungskette ist darauf ausgelegt, deterministische Latenzen und eine robuste Plausibilisierung der Objekte sicherzustellen.

Als Eingabe dient das Topic \texttt{/obstacle\_points} mit dem Nachrichtentyp \texttt{sensor\_msgs/PointCloud2} unter Verwendung des \emph{SensorDataQoS}-Profils. Als Ausgabe werden die geschätzten Objektboxen in einem \texttt{vision\_msgs/Detection3DArray} bereitgestellt, welches Zentrum, Ausmaße und optional die Orientierung umfasst. Zusätzlich erfolgt eine parallele Veröffentlichung als RViz-Marker zur visuellen Kontrolle. Für den Sensorpfad wird ein \emph{BestEffort}-Profil mit \emph{KeepLast} (Tiefe \(\leq 4\)) verwendet, während die Ergebnis-Topics zuverlässig (\emph{reliable}) übertragen werden. Eine harte Obergrenze \texttt{max\_clusters} sowie die Vorallokation der Speicherstrukturen verhindern Latenzspitzen bei dichter Punktbelegung.

Zur Unterdrückung von Rauschen und großflächigen Artefakten werden größenbasierte Filter eingesetzt. Objekte, deren geschätzte Ausmaße außerhalb plausibler Grenzen liegen – etwa extrem flache Fassaden oder unrealistisch große Boxen – werden verworfen. Die Parameter \(\varepsilon\), \texttt{min\_cluster\_size} und die Grenzen der Größenfilter sind direkt mit der Auflösung des Voxel-Downsamplings zu koppeln. Eine praxisnahe Faustregel ist \(\varepsilon \approx 2 \dots 3 \times \texttt{voxel\_size}\). Für die Schätzung der Höhe \(H\) eignen sich robuste Perzentilschätzer, um Ausreißer in vertikaler Richtung zu kompensieren.

Die Kombination aus Voxel-Downsampling, KdTree-Suche und einer festen Maximalzahl an Clustern ermöglicht eine stabile Verarbeitung in Echtzeit. Zudem wird bewusst auf Axis-Aligned Bounding Boxes (AABB) statt auf rechenintensivere OBBs zurückgegriffen, da AABBs bei Fahrzeugumgebungen als numerisch stabil und ausreichend präzise gelten. Insgesamt führt dieser Aufbau zu einer effizienten Segmentierung mit konstanten Antwortzeiten und gut kontrollierbaren Ressourcenanforderungen.

\section{Ergebnis}
\label{sec:ergebnis_cluster_boxes}
In den Experimenten liefert die Verarbeitungskette konsistente Boxen bei moderatem Rechenaufwand. Die AABB-Wahl erweist sich als robust gegenüber variabler Punktdichte und Teilokklusion; OBB bietet dagegen Vorteile bei stark orientierten, elongierten Objekten, ist jedoch empfindlicher und teurer.

Abbildung~\ref{fig:clustering_result} zeigt den Cluster eines detektierten Pkw mit AABB-Dimensionen.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/clustering_result.png}
  \caption{Detektierter Fahrzeug-Cluster mit AABB-Dimensionen (4.06 × 2.04 × 1.33 m)}
  \label{fig:clustering_result}
\end{figure}

\chapter{Objektverfolgung}
\section{Prinzip}

Zur Verfolgung erkannter Objekte wird ein zweidimensionaler Kalman-Filter mit anschließender Zuordnung der Detektionen mittels Ungarischem Algorithmus eingesetzt. Der Tracking-Zyklus folgt dabei einem festgelegten Schema: (1) \textbf{Prädiktion} aller bestehenden Tracks anhand eines Bewegungsmodells, (2) \textbf{Gating} neuer Detektionen über Mahalanobis-Distanzen, (3) \textbf{globale Zuordnung} per ungarischem Algorithmus und (4) \textbf{Track-Management}. Letzteres umfasst die Initialisierung neuer Spuren aus nicht zugeordneten Detektionen, das Fortführen bestehender Tracks ohne Messung für eine begrenzte \textit{Time-to-Live} sowie das Löschen überalterter Tracks. Durch die Beschränkung auf eine 2D-Bewegungsebene wird Rechenaufwand eingespart, während Höhenvariationen bereits in der Clustering-Stufe kontrolliert werden. Im Folgenden wird zunächst der Kalman-Filter und anschließend das Zuordnungsverfahren erläutert.

\subsection{2D-Kalman-Filter}
Der Kalman-Filter schätzt den Zustandsvektor eines Objekts und aktualisiert ihn bei jeder neuen Messung (\cite{kalman1960}). Im hier verwendeten 2D-Modell lautet der Zustandsvektor
\[
\mathbf{x} =
\begin{bmatrix}
x \\ y \\ v_x \\ v_y
\end{bmatrix},
\]
wobei $(x,y)$ die Position und $(v_x, v_y)$ die Geschwindigkeit in der Ebene beschreiben. Als Bewegungsmodell wird konstante Geschwindigkeit angenommen, d.\,h. die Position entwickelt sich proportional zur Geschwindigkeit, während die Geschwindigkeit unverändert bleibt. Dieses lineare Modell ist für Tracking-Aufgaben im Verkehrsumfeld weit verbreitet \cite{bar2004estimation}.

Die Zustandsvorhersage erfolgt mittels Übergangsmatrix
\[
\mathbf{F} =
\begin{bmatrix}
1 & 0 & \Delta t & 0\\
0 & 1 & 0 & \Delta t\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix},
\]
wobei $\Delta t$ die Zeitdifferenz zwischen zwei Messungen darstellt. Die Kovarianzmatrix wird entsprechend
\[
\mathbf{P}_{k|k-1} = \mathbf{F}\,\mathbf{P}_{k-1|k-1}\,\mathbf{F}^T + \mathbf{Q}
\]
fortgeschrieben, wobei $\mathbf{Q}$ das Prozessrauschen modelliert und Beschleunigungen oder Modellabweichungen abdeckt. In der Implementierung wird $\mathbf{Q}$ auf Basis der empirisch beobachteten Geschwindigkeitsänderungen gewählt (typisch $\sigma_a \approx 1\,\mathrm{m/s^2}$), um kurzfristige Manöver abzufangen, ohne dass der Filter instabil wird.

Liegt eine neue Detektion vor, wird der Filter aktualisiert. Die erwartete Messung wird über
\[
\mathbf{z}_{k|k-1} = \mathbf{H}\,\mathbf{x}_{k|k-1}, \quad
\mathbf{H} =
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{bmatrix}
\]
berechnet. Der Innovationsvektor lautet
\[
\mathbf{y}_k = \mathbf{z}_k - \mathbf{H}\,\mathbf{x}_{k|k-1},
\]
und mit der Innovationskovarianz
\[
\mathbf{S}_k = \mathbf{H}\,\mathbf{P}_{k|k-1}\,\mathbf{H}^T + \mathbf{R}
\]
ergibt sich der Kalman-Gewinn:
\[
\mathbf{K}_k = \mathbf{P}_{k|k-1}\,\mathbf{H}^T\,\mathbf{S}_k^{-1}.
\]

Damit wird der Zustand korrigiert zu
\[
\mathbf{x}_{k|k} = \mathbf{x}_{k|k-1} + \mathbf{K}_k\,\mathbf{y}_k,
\]
und die Unsicherheit reduziert sich entsprechend
\[
\mathbf{P}_{k|k} = (\mathbf{I} - \mathbf{K}_k\mathbf{H})\,\mathbf{P}_{k|k-1}.
\]

Die normierte Innovationsgröße
\[
d^2 = \mathbf{y}_k^T\,\mathbf{S}_k^{-1}\,\mathbf{y}_k
\]
entspricht dem Mahalanobis-Abstand und dient als statistisches Maß, ob eine Messung konsistent zu einem Track passt. Große Abweichungen führen zur Ablehnung der Zuordnung (\textit{Gating}) (\cite{bar2004estimation}). Als Gate wird ein $\chi^2$-Schwellwert (typisch $d^2 \leq 9$ für 95~\% Konfidenz in 2D) genutzt. Nicht geupdatete Tracks behalten ihre Prädiktion, ihre Kovarianz wächst jedoch, wodurch der Gate-Radius über die Zeit zunimmt und späte Wiedererkennungen erleichtert.

\subsection{Datenassoziation mit dem Ungarischen Algorithmus}

Um in jedem Zeitschritt neue Detektionen den bestehenden Objekt-Spuren (Tracks) korrekt zuzuordnen, wird ein Zuordnungsproblem gelöst. Hierfür wird zunächst eine Kostenmatrix
\[
\mathbf{C} \in \mathbb{R}^{N_\text{Track} \times N_\text{Detektion}}
\]
aufgebaut, deren Eintrag $c_{ij}$ die Kosten einer möglichen Zuordnung zwischen Track $i$ und Detektion $j$ beschreibt. In dieser Arbeit wird als Kostenmaß der quadratische Mahalanobis-Abstand verwendet,
\[
c_{ij} = (\mathbf{z}_j - \hat{\mathbf{z}}_i)^\mathrm{T}\, \mathbf{S}_i^{-1}\, (\mathbf{z}_j - \hat{\mathbf{z}}_i),
\]
wobei $\hat{\mathbf{z}}_i$ die vom Kalman-Filter vorhergesagte Position des Tracks und $\mathbf{S}_i$ die Innovationskovarianz ist \cite{bar2004estimation}. Kleine Werte stehen für hohe Übereinstimmung. 

Um unrealistische Paarungen auszuschließen, wird ein \textit{Gating} durchgeführt: liegt $c_{ij}$ oberhalb eines Schwellenwerts $\tau$, wird diese Zuordnung verworfen. Praktisch wird dies durch Setzen des Matrixeintrags auf einen sehr großen Wert erreicht:
\[
c_{ij} =
\begin{cases}
(\mathbf{z}_j - \hat{\mathbf{z}}_i)^\mathrm{T}\, \mathbf{S}_i^{-1}\, (\mathbf{z}_j - \hat{\mathbf{z}}_i), & \text{falls } c_{ij} \le \tau,\\
\infty, & \text{sonst}.
\end{cases}
\]
Dadurch werden nur Messungen berücksichtigt, die statistisch konsistent zum jeweiligen Track sind.

Im Anschluss bestimmt der Ungarische Algorithmus \cite{kuhn1955} eine optimale 1-zu-1-Zuordnung, welche die Summe der Kosten minimiert:
\[
\min_{\pi} \sum_{i} c_{i,\pi(i)}, \qquad \pi \text{ ist eine Permutation auf } \{1,\dots,N_\text{Track}\}.
\]
Das Verfahren garantiert eine eindeutige Zuordnung, bei der jeder Track höchstens eine Detektion erhält und jede Detektion höchstens einem Track zugewiesen wird. Tracks ohne passende Messung werden nur durch die Prädiktion weitergeführt, während nicht zugeordnete gültige Detektionen als neue Objekte initialisiert werden können. Durch diese Formulierung wird verhindert, dass mehrere Objekte derselben Detektion zugeordnet werden oder sich Objektidentitäten überschneiden – ein bekanntes Problem bei rein lokalem Nearest-Neighbor-Matching (\cite{bewley2016sort}).


\section{Implementierung}
\label{sec:implementierung_tracking}
Die Umsetzung erfolgt als ROS~2\,--\,Knoten \texttt{tracking\_node}. Eingänge sind \texttt{/detections\_raw} (\texttt{vision\_msgs/Detection3DArray}) aus der Clusterstufe, Ausgaben \texttt{/tracks\_raw} (Detektionen mit stabilisierten Zentren/Größen/IDs) sowie \texttt{/tracks\_markers} für RViz. Sensorpfade nutzen \emph{SensorDataQoS} (BestEffort/KeepLast), Ergebnis\,--\,Topics \emph{reliable}. 

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|p{8.5cm}|}
    \hline
    \textbf{Parameter} & \textbf{Typ} & \textbf{Bedeutung} \\ \hline
    \texttt{gate\_dist\_max} & [m] & euklidische Obergrenze des Gates (z.\,B. 3\,--\,4\,m) \\ \hline
    \texttt{min\_hits} & [Frame] & Bestätigungsschwelle (2\,--\,3) \\ \hline
    \texttt{max\_missed} & [Frame] & Löschschwelle ohne Zuordnung (6\,--\,10) \\ \hline
    \texttt{max\_size\_L,W,H} & [m] & Plausibilitätsobergrenzen (z.\,B. L\,\(\leq\,8\), W\,\(\leq\,5\), H\,\(\leq\,4.2\)) \\ \hline
  \end{tabular}
    \caption{Wesentliche Parameter des Trackers .}
    \label{tab:tracking_params}
  \end{table}

  Die Wahl der Startwerte folgt gängigen Empfehlungen aus der Tracking-Literatur. Für das \texttt{gate\_dist\_max} wird ein euklidischer Gate-Radius von rund 4~m genutzt, was bei typischen Positionsvarianzen ($\sigma \approx 1.5$--$2$~m) in etwa einem 95\,\%-Konfidenzintervall des Mahalanobis-Gates entspricht und damit den in \textcite{bar2004tracking,blackman1999design} empfohlenen Kompromiss zwischen Fehlassoziationen und Wiedererkennung abdriftender Tracks abbildet. \texttt{max\_missed}\,$=10$ erlaubt es einem Track, bei der hier verwendeten Sensorrate von 10~Hz etwa eine Sekunde ohne Messung zu überbrücken (z.\,B. bei kurzzeitigen Abschattungen), liegt damit im oberen Bereich der in \textcite{bewley2016sort} vorgeschlagenen Time-to-Live von 1~s, führt in Tests aber zu deutlich stabileren Trajektorien in dichtem Verkehr. \texttt{min\_hits}\,$=2$ stellt sicher, dass neue Spuren mindestens zweimal bestätigt werden müssen, bevor sie als valide Track-IDs ausgegeben werden; dies folgt der Heuristik von \textcite{bewley2016sort}, wonach eine niedrige Bestätigungsschwelle Reaktionsgeschwindigkeit wahrt, aber Einzelmessungen und Sensorartefakte unterdrückt.

  Die folgenden Größenbereiche (nach Bodenentfernung) dienen sowohl der Plausibilisierung als auch der heuristischen Klassenbestimmung. Sie wurden in den Versuchen verwendet und haben sich als robuste Grenzen für typische Straßenszenen erwiesen. Dabei wurde die Bodenentfernung betrachtet.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Objektklasse} & \textbf{Länge [m]} & \textbf{Breite [m]} & \textbf{Höhe [m]} \\ \hline
    Mensch         & 0.3 -- 1.6  & 0.4 -- 1.0  & 1.0 -- 2.0 \\ \hline
    Fahrradfahrer  & 0.6 -- 2.0  & 0.6 -- 1.5  & 0.4 -- 1.6 \\ \hline
    Pkw            & 2.0 -- 4.5  & 1.4 -- 3.0  & 0.6 -- 2.1 \\ \hline
    Lkw            & 4.5 -- 8.0  & 2.0 -- 5.0  & 2.0 -- 4.2 \\ \hline
  \end{tabular}
  \caption{Abmessungsbereiche der Zielklassen nach Bodenentfernung (Plausibilisierung und Heuristik).}
  \label{tab:class_dims}
\end{table}

Boxen, deren Abmessungen über globalen Obergrenzen liegen (\enquote{größer als Lkw}), werden vor Anlage/Aktualisierung verworfen; dies reduziert Fehlassoziationen und senkt Rechenlast. Außerdem werden nur vordefinierte Klassen (Person, Fahrradfahrer, Pkw, Lkw) getrackt; \enquote{Unbekannt} kann zur Unterdrückung von Artefakten verworfen werden.

Tracks werden als \texttt{vision\_msgs/Detection3D} ausgegeben (Zentrum, Größe, Orientierung, ID über Label/Namespace). Die Orientierung wird bei AABB aus der Detektion (z.\,B. BEV oder Cluster) übernommen, nicht aus dem KF geschätzt. Für Debug wird ein \texttt{MarkerArray} publiziert (Box und Textlabel). Abbildung~\ref{fig:tracking_flow} skizziert den Ablauf.

\begin{figure}[H]
  \centering
  % Platzhalter — später durch Ablaufgrafik ersetzen
  \fbox{\parbox{0.9\linewidth}{\centering
    Platzhalter: Datenfluss — Detections\,\(\to\) Gating\,/\,Zuordnung\,\(\to\) KF\,Update\,\(\to\) Track-Management\,\(\to\) Ausgabe.
  }}
  \caption{Ablauf der Objektverfolgung (Schemadarstellung).}
  \label{fig:tracking_flow}
\end{figure}

\section{Ergebnis}
\label{sec:ergebnis_tracking}
Mit den in Kapitel~\ref{chap:cluster_extraction} beschriebenen Boxen liefert der Tracker stabile Trajektorien bei moderatem Rechenaufwand. Die AABB\,--\,Wahl in der Boxstufe reduziert Formsprünge; in dichtem Verkehr hilft Mahalanobis\,--\,Gating, falsche Zuordnungen zu minimieren. Startwerte sind \texttt{gate\_dist\_max}\,\(\=4\) \text{m} \texttt{min\_hits}\,\(=2\) und \texttt{max\_missed}\,\(=10\).

Abbildung~\ref{fig:tracking_result} zeigt den Cluster eines detektierten Pkw mit AABB-Dimensionen.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{Bilder/tracking.png}
  \caption{Detektierte Objekte (Bild wird noch bearbeitet)}
  \label{fig:tracking_result}
\end{figure}

\chapter{Integration in der GUI}
\label{chap:integration_gui}

\section{Anforderungen: Bestehende Funktionen und notwendige Erweiterungen}

Die bestehende IFZN-GUI basiert auf der modularen und erweiterbaren Architektur nach Wendel \cite{Wendel2025} und stellt bereits eine vollständig funktionierende Bedienoberfläche für verschiedene Sensorsysteme bereit. Zu den vorhandenen Funktionen gehören:

\begin{itemize}
    \item eine stabile GUI-Struktur zur Steuerung und Überwachung von Sensoren,
    \item die Nutzung zentraler Bash-Skripte und Launch-Files zum Starten der Sensorik,
    \item die Unterstützung mehrerer Betriebsmodi (Embedded PC + Laptop, Laptop mit Sensoren, Laptop),
    \item grundlegende Funktionen wie Messdatenaufzeichnung, Wiedergabe von ROS~2-Bag-Dateien und Visualisierung in RViz,
    \item eine automatisierte Startlogik über MATLAB und \texttt{tmux} für bestehende Sensorketten.
\end{itemize}

Diese Infrastruktur bildet die Grundlage für die Integration des Ouster~OS1 und ermöglicht eine Erweiterung ohne grundlegende Anpassung der bestehenden Bedienphilosophie.

Zur vollständigen Integration des Ouster~OS1 in die GUI müssen zusätzliche Komponenten ergänzt werden, die in der ursprünglichen Anwendung nicht enthalten waren:

\begin{enumerate}
    \item \textbf{Einbindung des Ouster-Treibers:}  
    Ergänzung und Anpassung des \texttt{sensor.launch.xml}, um den Ouster-Treiber in die bestehende Startlogik einzubetten.

    \item \textbf{Integration der Nodes der Verarbeitungskette:}  
    Die im Rahmen dieser Arbeit entwickelte Verarbeitungskette erfordert die Einbindung folgender ROS~2-Nodes:
    \begin{itemize}
        \item \texttt{crop\_box\_node},
        \item \texttt{voxel\_filter\_node},
        \item \texttt{ground\_segmentation\_node},
        \item \texttt{cluster\_extraction\_node},
        \item \texttt{tracking\_node}.
    \end{itemize}

    Diese Nodes müssen im \texttt{sensor.launch.xml} (unter , in den Aufzeichnungsskripten (\texttt{run\_aufzeichnung}) sowie in der MATLAB-App (\texttt{LiDAR\_Embedded\_PC.mlapp}) berücksichtigt werden.

    \item \textbf{Erweiterung der Betriebsmodi:}  
    Die GUI muss die neue Ouster-basierte Verarbeitungskette in allen drei vorhandenen Modi starten können.

    \item \textbf{Automatisches Laden aller Abhängigkeiten:}  
    Alle nötigen Umgebungsvariablen (\texttt{setup.bash}, Pfade, Domain-ID) müssen für die erweiterten Startabläufe hinterlegt werden.
\end{enumerate}

Damit wird die neue Sensorkette vollständig in das vorhandene GUI-System integriert und in allen Betriebsmodi nutzbar.

\section{Umsetzung}

Die Integration des neuen Verarbeitungspakets \texttt{ouster\_cpp} in die bestehende IFZN-GUI erforderte mehrere gezielte Anpassungen an der Dateistruktur, den Workspaces sowie den Startmechanismen der ROS\,2-Nodes. Ziel war es, die gesamte Verarbeitungskette -- bestehend aus den Modulen \emph{CropBox}, \emph{VoxelGrid}, \emph{RANSAC-Bodensegmentierung}, \emph{Cluster-Extraktion} und \emph{Tracking} -- in allen vorgesehenen Betriebsmodi nutzbar zu machen:  
(1) Laptop,  
(2) Laptop mit Sensor,  
(3) Embedded-PC + Laptop.  
Im Folgenden werden die vorgenommenen Schritte beschrieben.

\subsection*{1.\ Vereinheitlichung der Workspace-Struktur}

Zu Beginn befand sich das neue Paket \texttt{ouster\_cpp} in einem separaten Workspace, der im Rahmen der GUI jedoch nicht berücksichtigt wurde. Da die IFZN-GUI ausschließlich den zentralen Workspace unter  
\texttt{Documents/Sensorik\_Astra/Ouster\_OS1/ros2\_ws}  
synchronisiert und aus diesem sowohl lokal als auch auf dem Embedded-PC alle relevanten Nodes startet, wurde das Paket vollständig in diesen Haupt-Workspace überführt.

Dies gewährleistet:
\begin{itemize}
  \item einen konsistenten Build-Prozess über alle Modi hinweg,
  \item die automatische Verfügbarkeit des Pakets auf dem Embedded-PC durch die GUI-Synchronisation,
  \item eine einheitliche Pfadstruktur für alle Startskripte.
\end{itemize}

\subsection*{2.\ Erweiterung des bestehenden Launch-Systems}

Damit die vollständige Verarbeitungskette gemeinsam mit dem Ouster-Treiber (\texttt{os\_driver}) gestartet werden kann, wurde die zentrale Launch-Datei \texttt{sensor.launch.xml} des Pakets \texttt{ouster\_ros} erweitert.  
Innerhalb des bestehenden Ouster-Namespaces wurde ein zusätzlicher Unter-Namespace \texttt{bench} angelegt und dort alle neuen Executables
\texttt{crop\_box\_node}, \texttt{voxel\_filter\_node}, \texttt{ransac\_ground\_node}, \texttt{cluster\_extraction\_node}, \texttt{tracking\_node}
als eigenständige \texttt{<node>}-Blöcke ergänzt.

Damit werden sämtliche Verarbeitungsschritte automatisch aktiviert, sobald die GUI den Ouster-Sensor startet -- unabhängig vom gewählten Betriebsmodus.

\subsection*{3.\ Anpassungen an den GUI-Startskripten}

Die GUI nutzt mehrere tmux-basierte Startskripte, welche die benötigten Nodes abhängig vom Betriebsmodus ausführen.  
Um die Funktionsfähigkeit der erweiterten Verarbeitungskette sicherzustellen, wurden folgende Anpassungen vorgenommen:

\begin{itemize}
  \item \textbf{Laptop- und Laptop-mit-Sensor-Modus:}  
  Das Skript \texttt{run\_ouster\_os1.bash} wurde so belassen, dass es den zentralen Workspace kompiliert und anschließend die erweiterte \texttt{sensor.launch.xml} ausführt. Somit laufen der Ouster-Treiber und alle \texttt{ouster\_cpp}-Nodes lokal.

  \item \textbf{Embedded-PC + Laptop:}  
  Da die GUI beim Aufbau der Verbindung den zentralen Workspace automatisch auf den Embedded-PC überträgt, stehen dort alle Nodes ohne weitere Änderungen zur Verfügung. Beim Start werden dieselben Launch-Dateien via~SSH ausgeführt, sodass die identische Verarbeitungskette auf dem Embedded-PC ausgeführt wird.

  \item \textbf{Aufzeichnungsmodus:}  
  Auch im Skript \texttt{run\_aufzeichnungen.sh} wird der zentrale Workspace kompiliert und anschließend ein Launch-File (benchmark.launch.py) ausgeführt. Durch die Integration von \texttt{ouster\_cpp} in den Haupt-Workspace stehen alle Nodes ebenfalls für Aufzeichnungen bereit, ohne dass die Skripte separat erweitert werden mussten.
\end{itemize}

\subsection*{4.\ Sicherstellung der ROS\,2-Kompatibilität}

Damit die GUI alle relevanten Nodes zuverlässig erkennen und überwachen kann, wurden folgende strukturelle Anforderungen erfüllt:

\begin{itemize}
  \item alle Executables sind über \texttt{install(TARGETS ...)} korrekt in den \texttt{install/}-Baum eingebunden,
  \item sämtliche benötigten Abhängigkeiten (\texttt{rclcpp}, \texttt{sensor\_msgs}, \texttt{pcl\_conversions}, \texttt{vision\_msgs}, \texttt{Eigen3} usw.) sind im \texttt{package.xml} hinterlegt,
  \item konsistente Namespaces (\texttt{/ouster/bench/...}) ermöglichen eine eindeutige Identifikation der Nodes mittels \texttt{ros2 node list}.
\end{itemize}

\section{Ergebnis und mögliche Ursachen der Einschränkungen}

Die Integration des Verarbeitungspakets \texttt{ouster\_cpp} konnte für die Betriebsmodi \emph{Laptop} sowie \emph{Laptop mit Sensor} erfolgreich umgesetzt werden. In diesen Fällen wurden sämtliche Nodes korrekt kompiliert, durch die erweiterten Launch-Dateien gestartet und von der GUI erkannt, sodass die vollständige Verarbeitungskette funktionsfähig war.

Im Modus \emph{Embedded-PC + Laptop} zeigte sich jedoch eine Einschränkung: Obwohl der zentrale Workspace korrekt auf den Embedded-PC übertragen wurde und die Nodes technisch startfähig waren, wurde der Ouster-Sensor von der GUI nicht als vollständig aktiviert erkannt. Dadurch konnte der Betriebsmodus nicht in den regulären Zustand \enquote{aktiv} übergehen.

Eine mögliche Ursache für dieses Verhalten könnte in der internen Validierungslogik der GUI liegen. In der Klasse \texttt{LiDAR\_Embedded\_PC} wird für jeden Sensor eine Liste erwarteter Node-Namen geführt, beispielsweise
\begin{quote}
\texttt{ousteros1nodes = \{'/ouster/os\_driver'\}}.
\end{quote}
Diese Liste dient dazu, sicherzustellen, dass der Sensortreiber korrekt läuft. Für die erweiterten Verarbeitungsschritte des Pakets \texttt{ouster\_cpp} wurden zusätzliche Nodes wie
\texttt{/ouster/bench/crop\_box\_node\_cpp},
\texttt{/ouster/bench/voxel\_filter\_node\_cpp},
\texttt{/ouster/bench/ransac\_ground\_node\_cpp},
\texttt{/ouster/bench/cluster\_extraction\_node\_cpp}
und
\texttt{/ouster/bench/tracking\_node\_cpp}
ergänzt.

Es ist denkbar, dass die Einbindung dieser zusätzlichen Nodes in die GUI-interne Prüfliste dazu führte, dass der Embedded-PC-Modus die Aktivierung des Sensors nicht mehr bestätigte. Mögliche Gründe hierfür könnten sein:
\begin{itemize}
    \item zeitlich verzögertes Starten einzelner Nodes über die SSH-Verbindung,
    \item unterschiedliche oder verschachtelte Namespaces, welche von der GUI nicht erwartungsgemäß ausgewertet werden,
    \item oder ein generelles Missverhältnis zwischen der ursprünglichen GUI-Logik (ausgelegt auf genau eine Node pro Sensor) und einer erweiterten Verarbeitungskette mit mehreren abhängigen Nodes.
\end{itemize}

Es handelt sich dabei um Hypothesen, die das beobachtete Verhalten plausibel erklären könnten; sie konnten im Rahmen der Arbeit jedoch nicht abschließend verifiziert werden. Entscheidend ist, dass die technische Integration des Pakets \texttt{ouster\_cpp} selbst erfolgreich war und die Einschränkungen ausschließlich die GUI-Validierung im Modus \emph{Embedded-PC + Laptop} betreffen.

Eine mögliche Weiterentwicklung der GUI könnte darin bestehen, die Aktivitätsprüfung flexibler zu gestalten oder lediglich die Minimalmenge notwendiger Nodes (z.\,B.\ den Treiber \texttt{os\_driver}) zu überprüfen, um komplexere Verarbeitungspipelines künftig vollständig zu unterstützen.

\chapter{Test und Bewertung}
\label{chap:test_und_bewertung}

In diesem Kapitel werden die Testergebnisse der entwickelten Umfelderkennungs-Verarbeitungskette vorgestellt und bewertet. 
Die Verarbeitungskette wurde vollständig in ROS~2 umgesetzt und besteht aus den in Abbildung~\ref{fig:ros2-pipeline-alltopics} gezeigten Modulen: 
\textit{Crop-Box-Filter}, \textit{VoxelGrid-Filter}, \textit{RANSAC-Bodensegmentierung}, \textit{Cluster-Extraktion} sowie \textit{Objektverfolgung (Tracking)}. 
Die Konfiguration erfolgte über das Launch-File \textit{ouster\_pipeline.launch.py}, welches alle Module innerhalb des gemeinsamen Namensraums \textit{/pipeline} startet.

Das System lief auf einem Ubuntu~22.04-Rechner mit ROS~2~Humble und einem AMD Ryzen 5 Prozessor (8~Kerne, 16~Threads). 
Zur Visualisierung und qualitativen Analyse wurde \textit{RViz} eingesetzt.  

\section{Parameterstudie}
\label{sec:parameterstudie}

Die gewählte Detektionspipeline ist sensitiv gegenüber mehreren Parametern, die die Qualität der Segmentierung, Clusterbildung und Klassentrennung maßgeblich beeinflussen. Ziel der Parameterstudie ist daher die Untersuchung, welche Kombinationen aus \textit{voxel\_size}, \textit{distance\_threshold} und \textit{cluster\_tolerance} robuste Ergebnisse in unterschiedlichen urbanen Szenarien liefern. Zu diesem Zweck wurden aufgezeichnete ROS2-Bagfiles mehrfach wiederholt abgespielt und jeweils nur ein Parameter verändert, während alle anderen konstant gehalten wurden. Dadurch lässt sich der isolierte Einfluss jeder Stellgröße bewerten.

Die Datensätze für die Parametrisierung wurden bei 9\,°C und bedecktem Himmel aufgezeichnet, sodass keine direkten Wettereinflüsse durch starke Sonneneinstrahlung oder Niederschlag zu erwarten waren.

Im Fokus stehen bewusst nur diese drei Stellgrößen, weil sie die größten Hebel auf die Verarbeitungsqualität besitzen und jeweils unterschiedliche Stufen der Pipeline adressieren: \textit{voxel\_size} steuert die Ausdünnung der Punktwolke (Rauschunterdrückung versus Detailtreue), \textit{distance\_threshold} legt die Inlier-Schwelle der Bodensegmentierung fest und \textit{cluster\_tolerance} bestimmt die räumliche Nachbarschaft beim Clustering. Andere Parameter wurden nicht variiert, da sie entweder stark durch die Versuchskonfiguration vorgegeben sind (\texttt{min\_bound}/\texttt{max\_bound} der CropBox definieren den vermessenen Sichtbereich), bereits konservative Robustheitsreserven bieten (\texttt{max\_iterations} der RANSAC-Suche), nur sekundären Einfluss auf die Klassentrennung haben (\texttt{min\_cluster\_size}, \texttt{max\_cluster\_size}, \texttt{max\_clusters}, \texttt{bbox\_type}) oder die Stabilität der Track-IDs betreffen und damit die Vergleichbarkeit der Wiederholungsmessungen erschweren würden (\texttt{gate\_dist\_max}, \texttt{max\_missed}, \texttt{min\_hits}). Auf diese Weise bleibt der Suchraum handhabbar, ohne zentrale Qualitätstreiber der Pipeline zu vernachlässigen.

\subsection{Untersuchungsaufbau}
Die Messungen wurden in drei realen Szenarien durchgeführt:

\begin{itemize}
    \item \textbf{S1 – Geparkte Fahrzeuge auf einer ebenen Fahrbahn:} statische Umgebung mit eng stehenden PKW, geringer Verkehr, spiegelnde Oberflächen, hohe Gefahr von Übersegmentierung.
    \item \textbf{S2 – Kreuzung:} Objekte in dichter Nachbarschaft, teilweise verdeckt, dynamische Szene; Ziel: Minimierung von Falsch-Detektionen und stabiler Klassentrennung.
    \item \textbf{S3 – Unebene Fahrbahn:} geneigte Fläche mit parkenden Fahrzeugen; bestätigt Robustheit der Bodensegmentierung und der Clusterbildung bei variierender Höhengeometrie.
\end{itemize}

\begin{table}[H]
  \centering
  \caption{Übersicht der verwendeten Messaufzeichnungen}
  \begin{tabular}{|c|l|l|l|}
    \hline
    \textbf{Szenario} & \textbf{Beschreibung} & \textbf{Messaufzeichnung} & \textbf{Zeitraffer} \\ \hline
    S1 & Geparkte Autos & 2025-10-15-VK-OL-006 & Start: 5s; run 10s \\ \hline
    S2 & Kreuzung & 2025-10-15-VK-OL-007 & Start: 20s; run 10s \\ \hline
    S3 & Unebene Fläche & 2025-10-15-VK-OL-004 & Start: 145s; run 10s \\ \hline
  \end{tabular}
\end{table}

Als Bodenwahrheit (``Realwert'') dienten zum einen Kameraaufnahmen im Vorderbereich, zum anderen das 360° LiDAR-Datenbild für rückwärtige Objekte. Bewertet wurden jeweils die Klassen \textit{PKW}, \textit{Mensch}, \textit{Fahrradfahrer} und \textit{LKW} sowie die Gesamtdetektionen.

Die in Tabelle~\ref{tab:startwerte_parameter} aufgeführten Werte blieben während der Variation von \textit{voxel\_size} unverändert und dienen als Referenz für die folgenden Auswertungen.

\begin{table}[H]
  \centering
  \caption{Startwerte der Parameter vor der Parametrisierung}
  \label{tab:startwerte_parameter}
  \begin{tabular}{lll}
    \toprule
    \textbf{Node} & \textbf{Parameter} & \textbf{Wert} \\
    \midrule

    \texttt{crop\_box\_node\_cpp} \\
      & \texttt{min\_bound}  & \([-10.0,\ -6.0,\ -3.0]\) \\
      & \texttt{max\_bound}  & \([10.0,\ 6.0,\ 5.0]\) \\
    \midrule

    \texttt{ransac\_ground\_node\_cpp} \\
      & \texttt{distance\_threshold} & 0.15 \\
      & \texttt{max\_iterations}    & 1000 \\
    \midrule

    \texttt{cluster\_extraction\_node\_cpp} \\
      & \texttt{cluster\_tolerance} & 0.50 \\
      & \texttt{min\_cluster\_size} & 40 \\
      & \texttt{max\_cluster\_size} & 8000 \\
      & \texttt{max\_clusters}      & 200 \\
      & \texttt{bbox\_type}         & aabb \\
    \midrule

    \texttt{tracking\_node\_cpp} \\
      & \texttt{gate\_dist\_max} & 4.0 \\
      & \texttt{max\_missed}    & 10 \\
      & \texttt{min\_hits}      & 2 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{S1: Geparkte Fahrzeuge auf einer ebenen Fahrbahn}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{voxel\_size} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 12 & 3 & 3 & 0 & 18 \\
0.15 & 15 & 1 & 3 & 2 & 21 \\
0.20 & \textbf{15} & \textbf{0} & \textbf{1} & \textbf{1} & \textbf{17} \\
0.25 & 15 & 0 & 1 & 2 & 18 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche voxel\_size-Werte (Geparkte Autos).}
\label{tab:geparkt_voxel}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{distance\_threshold} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 16 & 0 & 1 & 1 & 18 \\
0.15 & 15 & 0 & 1 & 1 & 17 \\
0.20 & \textbf{17} & \textbf{0} & \textbf{1} & \textbf{2} & \textbf{20} \\
0.25 & 16 & 0 & 1 & 1 & 18 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche distance\_threshold-Werte (Geparkte Autos, voxel\_size = 0.20\,m).}
\label{tab:geparkt_distance}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{cluster\_tolerance} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.30 & 19 & 0 & 4 & 1 & 24 \\
0.50 & \textbf{17} & \textbf{0} & \textbf{1} & \textbf{1} & \textbf{19} \\
0.70 & 8  & 0 & 1 & 2 & 11 \\
0.90 & 7  & 0 & 1 & 2 & 10 \\
\hline
Real & 15 & 0 & 1 & 3 & 19 \\
\hline
\end{tabular}
\caption{Detektionen pro Klasse für unterschiedliche Werte der cluster\_tolerance (Geparkte Autos, voxel\_size = 0{,}15\,m, distance\_threshold = 0{,}20\,m).}
\label{tab:geparkt_cluster_tolerance}
\end{table}

\subsection{S2 – Kreuzung}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche Voxelgrößen (Kreuzung).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{voxel\_size (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.10 & 11 & 13 & 2 & 1 & 27 \\
    0.15 & 13 & 12 & 1 & 1 & 27 \\
    0.20 & 13 &  2 & 0 & 1 & 16 \\
    0.25 & 14 &  0 & 0 & 1 & 15 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_voxel_size}
\end{table}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche \texttt{distance\_threshold}-Werte (Kreuzung, voxel\_size = 0.15\,m).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{distance\_threshold (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.10 & 15 & 12 & 1 & 1 & 29 \\
    0.15 & 14 & 12 & 1 & 1 & 28 \\
    0.20 & 14 &  9 & 1 & 1 & 25 \\
    0.25 & 14 &  9 & 1 & 1 & 25 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_distance_threshold}
\end{table}

\begin{table}[H]
  \centering
  \caption{Anzahl Detektionen pro Klasse für unterschiedliche \texttt{cluster\_tolerance}-Werte (Kreuzung, voxel\_size = 0.15\,m, distance\_threshold = 0.20\,m).}
  \begin{tabular}{|l|c|c|c|c|c|}
    \hline
    \textbf{cluster\_tolerance (m)} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
    \hline
    0.30 &  8 &  8 & 4 & 1 & 21 \\
    0.50 & 14 &  9 & 1 & 1 & 25 \\
    0.70 & 17 &  8 & 1 & 1 & 27 \\
    0.90 & 16 &  8 & 1 & 1 & 26 \\
    \hline
    Real &  9 &  3 & 1 & 1 & 14 \\
    \hline
  \end{tabular}
  \label{tab:kreuzung_cluster_tolerance}
\end{table}

\subsection{S3 – Unebene Fahrbahn}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{voxel\_size} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 15 & 7 & 3 & 0 & 25 \\
0.15 & 15 & 7 & 2 & 1 & 25 \\
0.20 & 15 & 1 & 2 & 1 & 19 \\
0.25 & 16 & 0 & 2 & 1 & 19 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene voxel\_size (Unebene Fläche).}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{distance\_threshold} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.10 & 18 & 1 & 2 & 1 & 22 \\
0.15 & 15 & 1 & 2 & 1 & 19 \\
0.20 & 14 & 1 & 3 & 1 & 19 \\
0.25 & 14 & 1 & 3 & 1 & 19 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene distance\_threshold (Unebene Fläche).}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c|c|c|c|c|c}
\hline
\textbf{cluster\_tolerance} & \textbf{PKW} & \textbf{Mensch} & \textbf{Fahrradfahrer} & \textbf{LKW} & \textbf{Gesamt} \\
\hline
0.30 & 16 & 3 & 5 & 1 & 25 \\
0.50 & 16 & 1 & 2 & 1 & 20 \\
0.70 & 17 & 1 & 1 & 2 & 21 \\
0.90 & 13 & 1 & 0 & 2 & 16 \\
\hline
Real & 13 & 0 & 1 & 5 & 19 \\
\hline
\end{tabular}
\caption{Anzahl Detektionen pro Klasse für verschiedene cluster\_tolerance (Unebene Fläche).}
\end{table}

\subsection{Fazit der Parameterstudie}


\section{Test im Fahrbetrieb}

\subsection{Testumgebung}
\label{sec:testumgebung}

Für die Evaluierung der entwickelten Messkette wurden eigens aufgezeichnete LiDAR-Datensätze des Ouster~OS1-Sensors verwendet.
Die Aufnahmen erfolgten in verschiedenen urbanen Szenarien auf dem Campusgelände der Technischen Hochschule Nürnberg,
darunter Parkflächen, Zufahrtsstraßen und Bereiche mit Fußgänger- und Fahrzeugverkehr. Am Testtag herrschten \(-1\,^{\circ}\text{C}\)
mit gefühlten \(-4\,^{\circ}\text{C}\) bei überwiegend bewölktem Himmel.
Ziel dieser Datensätze war es, typische Objekte und Strukturen einer realen Verkehrsumgebung –
wie Personen, Fahrradfahrer, Pkw und Lkw – unter realistischen Bedingungen zu erfassen.
Die Sensordaten wurden mit einer festen Sensorrate von 10\,Hz aufgenommen und als \textit{ROS\,2\,Bag}-Dateien gespeichert.

\begin{table}[H]
  \centering
  \caption{Parameterübersicht der Nodes im \texttt{benchmark.launch.py} für den Test im Fahrbetrieb}
  \label{tab:benchmark_node_parameter_clean}
  \begin{tabular}{lll}
    \toprule
    \textbf{Node} & \textbf{Parameter} & \textbf{Wert} \\
    \midrule

    \texttt{crop\_box\_node\_cpp} 
      & \texttt{min\_bound}  & \([-10.0,\ -6.0,\ -3.0]\) \\
      & \texttt{max\_bound}  & \([10.0,\ 6.0,\ 5.0]\) \\
    \midrule

    \texttt{voxel\_filter\_node\_cpp} 
      & \texttt{voxel\_size} & 0.20 \\
    \midrule

    \texttt{ransac\_ground\_node\_cpp} 
      & \texttt{distance\_threshold} & 0.15 \\
      & \texttt{max\_iterations}    & 1000 \\
    \midrule

    \texttt{cluster\_extraction\_node\_cpp} 
      & \texttt{cluster\_tolerance} & 0.50 \\
      & \texttt{min\_cluster\_size} & 40 \\
      & \texttt{max\_cluster\_size} & 8000 \\
      & \texttt{max\_clusters}      & 200 \\
      & \texttt{bbox\_type}         & AABB \\
    \midrule

    \texttt{tracking\_node\_cpp} 
      & \texttt{gate\_dist\_max} & 4.0 \\
      & \texttt{max\_missed}    & 10 \\
      & \texttt{min\_hits}      & 2 \\
    \bottomrule
  \end{tabular}
\end{table}

Die im Fahrbetrieb gemessenen Kenngrößen zu Ressourcennutzung, Latenz sowie Eingangs- und Ausgangsfrequenzen der Verarbeitungskette sind in Tabelle~\ref{tab:fahrbetrieb_ueberblick} zusammengefasst.

\begin{table}[H]
  \centering
  \footnotesize
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{0.95}
  \begin{tabular}{|p{3.4cm}|p{3.1cm}|p{2.1cm}|p{2.1cm}|p{2.1cm}|}
    \hline
    \textbf{Szenario} & \textbf{CPU / RSS} & \textbf{mittlere Latenz} & \textbf{f\_in} & \textbf{f\_out} \\
    \hline

    Gerade Strecke, Stand &
    32\,\% / 197\,MB &
    86\,ms (stabil 81\,ms) &
    2.5\,Hz &
    1.8\,Hz \\
    \hline

    Kurve, 40\,km/h (Urban) &
    31.4\,\% / 198\,MB &
    39\,ms &
    3.7\,Hz &
    1.8\,Hz \\
    \hline

    Innenstadt + Steigung, 30\,km/h &
    12.3\,\% / 200\,MB &
    30\,ms &
    3.3--3.4\,Hz &
    2\,Hz \\
    \hline

    Kreuzung, 50\,km/h &
    n.\,a. &
    21--24\,ms &
    3.21--3.24\,Hz &
    kurze Einbrüche bis \(\approx 3{,}15\,\text{Hz}\)  \\
    \hline

    Urban, 50\,km/h (Unebene Fläche) &
    11.7\,\% / 200\,MB &
    20--26\,ms &
    3.2\,Hz &
    3.0\,Hz \\
    \hline
  \end{tabular}
  \caption{Vergleichende Übersicht der Messgrößen je Szenario}
  \label{tab:fahrbetrieb_ueberblick}
\end{table}

\section{Bewertung des Algorithmus}


% ====== 8 Zusammenfassung und Ausblick ======
\chapter{Zusammenfassung und Ausblick}
% Inhalt hier ergÃ¤nzen
\section{Zusammenfassung}
\section{Ausblick}

% ====== Literaturverzeichnis ======
\cleardoublepage
\printbibliography[title=Literaturverzeichnis]

% ====== Anhang ======
\appendix
\chapter{Quellcodeauszug CropBox-Node}
Die vollständige C++-Implementierung des CropBox-Filters ist in Listing~\ref{lst:cropbox_node}
aufgeführt.

\label{LastPage}
\end{document}

